{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP 2: RNNs\n",
        "This notebook is based on fastai's **[Chapter 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb)**.\n",
        "\n",
        "Please read that chapter before looking at this review.\n",
        "\n",
        "*I suggest opening this notebook in Colab (where it can be easier to use GPU).*\n",
        "*If you want to run it locally, set up the **deep-learning** environment in your terminal with `conda env create -f environment.yml` and activate it in your preferred IDE.*"
      ],
      "metadata": {
        "id": "JWmgr3zomfkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "82EFImrJaM14"
      },
      "outputs": [],
      "source": [
        "### FOR COLAB USERS ###\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### FOR LOCAL USERS ###\n",
        "# import fastai\n",
        "# print(fastai.__version__)\n",
        "\n",
        "# ! pip install -Uqq fastbook\n",
        "# import fastbook\n",
        "# fastbook.setup_book()"
      ],
      "metadata": {
        "id": "Chj3C4Lzm7VO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastbook import *\n",
        "from fastai.text.all import *"
      ],
      "metadata": {
        "id": "Y70NzsSdaSei"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "XxnSDe4oavYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "t7d2RD3QZSXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path\n",
        "path = untar_data(URLs.HUMAN_NUMBERS)\n",
        "Path.BASE_PATH = path\n",
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "jXeJmlLfaSb7",
        "outputId": "f8f42996-4ca0-4ae2-d891-3312291cac44"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='32768' class='' max='30252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      108.32% [32768/30252 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [Path('train.txt'),Path('valid.txt')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show an example\n",
        "lines = L()\n",
        "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
        "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
        "lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VO7COYKaSZE",
        "outputId": "08d896f8-3221-4ced-c3d4-14a52ede741e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first 100 characters\n",
        "text = ' . '.join([l.strip() for l in lines])\n",
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i3MDQmjZaSWN",
        "outputId": "84a0bf1f-cb51-4776-f1a3-c69720c8807b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "A_6sfnykZP0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize -- split on spaces\n",
        "tokens = text.split(' ')\n",
        "tokens[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgRdQfEjaSTm",
        "outputId": "9c5d56b7-df1e-426a-d675-30243b0f33f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create vocab (all unique tokens)\n",
        "vocab = L(*tokens).unique()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqHijsJLaSQf",
        "outputId": "b2eab95d-200a-4a7d-d2e6-87d58075e1f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token map (come up with index for where the token is in the vocab above)\n",
        "token_map = {w:i for i,w in enumerate(vocab)}\n",
        "token_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXewwQ2NaSN3",
        "outputId": "24378a57-2184-43e1-8e5e-e40167de9cf6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'one': 0,\n",
              " '.': 1,\n",
              " 'two': 2,\n",
              " 'three': 3,\n",
              " 'four': 4,\n",
              " 'five': 5,\n",
              " 'six': 6,\n",
              " 'seven': 7,\n",
              " 'eight': 8,\n",
              " 'nine': 9,\n",
              " 'ten': 10,\n",
              " 'eleven': 11,\n",
              " 'twelve': 12,\n",
              " 'thirteen': 13,\n",
              " 'fourteen': 14,\n",
              " 'fifteen': 15,\n",
              " 'sixteen': 16,\n",
              " 'seventeen': 17,\n",
              " 'eighteen': 18,\n",
              " 'nineteen': 19,\n",
              " 'twenty': 20,\n",
              " 'thirty': 21,\n",
              " 'forty': 22,\n",
              " 'fifty': 23,\n",
              " 'sixty': 24,\n",
              " 'seventy': 25,\n",
              " 'eighty': 26,\n",
              " 'ninety': 27,\n",
              " 'hundred': 28,\n",
              " 'thousand': 29}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numericalize\n",
        "nums = L(token_map[i] for i in tokens)\n",
        "nums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGyyovTY5nVu",
        "outputId": "3b3e1ed4-ce84-44c4-c780-5ca9cc755bd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aside: Embeddings\n",
        "- Embeddings = a simple lookup table that stores embeddings of a fixed dictionary and size\n",
        "- Can think of this sort of like a dictionary"
      ],
      "metadata": {
        "id": "_XvSiUnB9bNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a vocab of 5 words"
      ],
      "metadata": {
        "id": "cdRIyEL7irUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab\n",
        "tmp_vocab = {\"test\":0, \"example\":1, \"evie\":2, \"cat\":3, \"dog\":4}\n",
        "tmp_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_yFOng2-zel",
        "outputId": "f64a8e9a-55cc-4c48-add5-acaa7084bbe1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': 0, 'example': 1, 'evie': 2, 'cat': 3, 'dog': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create embedding structure\n",
        "This is sort of like a dictionary with keys = indices, values = embeddings"
      ],
      "metadata": {
        "id": "eORb8Ni19hEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up parameters\n",
        "vocab_size = len(tmp_vocab) # number of tokens (here, 5 words in the vocab)\n",
        "embedding_dim = 3 # number of dimensions for each token's embedding (here, 3 dimensions)"
      ],
      "metadata": {
        "id": "KYITFqwgi38-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create embedding structure\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zESUlTXC9dgE",
        "outputId": "f153820a-3cb1-4733-ba55-a50535b45c52"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get embeddings\n",
        "These embeddings are random values, since we did not give it pretrained values"
      ],
      "metadata": {
        "id": "vb8qgrZNjJP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get indices for two tokens in the vocab\n",
        "tok1 = tmp_vocab['evie']\n",
        "tok2 = tmp_vocab['cat']\n",
        "tok1, tok2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2SCle9NkbgB",
        "outputId": "c3bd778b-6942-4a16-fa2e-93ba50a76e7e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input = list of indices (sort of like a dict key)\n",
        "input = torch.LongTensor([2,3]) # tokens with indices 2 and 3\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocx7Dvf9j7_q",
        "outputId": "6a757e67-bf6c-408e-9e41-2cbc14d6578c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output = corresponding word embeddings (sort of like a dict value)\n",
        "output = embedding(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lb5as7I_Bme",
        "outputId": "97e36828-8eff-4373-d55f-3f07cf4c0147"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.2082, -0.6380,  0.4617],\n",
              "        [ 0.2674,  0.5349,  0.8094]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get embeddings using pretrained weights"
      ],
      "metadata": {
        "id": "5ivObFEo9itW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pretrained embedding weights for the tokens in the vocab"
      ],
      "metadata": {
        "id": "y3Ds21vOlVBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# there is one \"row\" per token (5 \"rows\"), with 3 dimension each (\"columns\")\n",
        "weight = torch.FloatTensor(\n",
        "  [\n",
        "    [0.0, 5.0, 10.0],\n",
        "    [1.0, 6.0, 11.0],\n",
        "    [2.0, 7.0, 12.0],\n",
        "    [3.0, 8.0, 13.0],\n",
        "    [4.0, 9.0, 14.0],\n",
        "  ]\n",
        ")\n",
        "print(weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ygF5lpb9eu8",
        "outputId": "5560ab98-49f0-4fed-90db-46ae2d1d3c1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  5., 10.],\n",
            "        [ 1.,  6., 11.],\n",
            "        [ 2.,  7., 12.],\n",
            "        [ 3.,  8., 13.],\n",
            "        [ 4.,  9., 14.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the embeddings\n",
        "Essentially, create a dictionary where keys are the indices and weights are the values"
      ],
      "metadata": {
        "id": "wVs9QksNlcjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding.from_pretrained(weight)\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeF-EGKPAZhP",
        "outputId": "c584f821-c44f-4653-e520-a1c2f29c2298"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get embeddings for tokens with index 2 and 3 in the vocab\n",
        "These embeddings come directly from the pretrained weights"
      ],
      "metadata": {
        "id": "agE9wQjPlp1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input = list of indices (sort of like a dict key)\n",
        "input = torch.LongTensor([2,3])\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xf_NxANlw3p",
        "outputId": "e3e3db91-0a58-4c96-8e46-f3c48645a286"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output = corresponding word embeddings (sort of like a dict value)\n",
        "output = embedding(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mcOrSxkAjKL",
        "outputId": "96b9af7b-8bb7-495c-86ef-92f108d5f27e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.,  7., 12.],\n",
              "        [ 3.,  8., 13.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary"
      ],
      "metadata": {
        "id": "q-en14Vvl9AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings for \"evie\"\n",
        "word_index = tmp_vocab['evie']\n",
        "word_index = torch.LongTensor([word_index]) # make it a tensor\n",
        "embedding(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4QaHrzr9pyi",
        "outputId": "21ad2044-126a-46ed-8c87-33a1df72436a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.,  7., 12.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Language Model\n",
        "- Goal = predict each word based on the previous 3 words\n",
        "- Inputs = 3 words\n",
        "- Outputs = Probability of each possible next word (from the vocab)"
      ],
      "metadata": {
        "id": "PUapLhGTlDLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of inputs and outputs\n",
        "- x = 3 tokens\n",
        "- y = 1 token"
      ],
      "metadata": {
        "id": "cTaSbiZBabcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# small sample\n",
        "toks = tokens[0:25]\n",
        "print(\"index \\t\\t input \\t\\t\\t output\".upper())\n",
        "\n",
        "# get sample inputs, the first input's token index, and true outputs\n",
        "for i in range(0,len(toks)-4,3):\n",
        "  x = toks[i:i+3]\n",
        "  y = toks[i+3]\n",
        "  print(i, \"\\t\", x, \"\\t\\t\", [y])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwRaor1o-Woh",
        "outputId": "8579814c-8e27-43bd-c0a5-af1fc06fe4c4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INDEX \t\t INPUT \t\t\t OUTPUT\n",
            "0 \t ['one', '.', 'two'] \t\t ['.']\n",
            "3 \t ['.', 'three', '.'] \t\t ['four']\n",
            "6 \t ['four', '.', 'five'] \t\t ['.']\n",
            "9 \t ['.', 'six', '.'] \t\t ['seven']\n",
            "12 \t ['seven', '.', 'eight'] \t\t ['.']\n",
            "15 \t ['.', 'nine', '.'] \t\t ['ten']\n",
            "18 \t ['ten', '.', 'eleven'] \t\t ['.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same example with the numericalized version\n",
        "n = nums[0:25]\n",
        "print(\"index \\t input \\t\\t\\t output\".upper())\n",
        "\n",
        "for i in range(0,len(n)-4,3):\n",
        "  x = n[i:i+3]\n",
        "  y = n[i+3]\n",
        "  print(i, \"\\t\", x, \"\\t\\t\", [y])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNOBTr1P-at7",
        "outputId": "bb160f78-6734-45f4-fa0b-3c2cf2fe6812"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INDEX \t INPUT \t\t\t OUTPUT\n",
            "0 \t [0, 1, 2] \t\t [1]\n",
            "3 \t [1, 3, 1] \t\t [4]\n",
            "6 \t [4, 1, 5] \t\t [1]\n",
            "9 \t [1, 6, 1] \t\t [7]\n",
            "12 \t [7, 1, 8] \t\t [1]\n",
            "15 \t [1, 9, 1] \t\t [10]\n",
            "18 \t [10, 1, 11] \t\t [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same numericalized example, just as tensors\n",
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
        "seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltVaDIHG-arX",
        "outputId": "698b50f5-19f3-4a5e-8b57-892e2eb81cd6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "z-WW_yitcXlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaders\n",
        "bs = 10 # small batch size for now\n",
        "cut = int(len(seqs) * 0.8) # where to split into train/valid sets (randomly, for now)\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, shuffle=False)\n",
        "dls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrInyh_RA_xY",
        "outputId": "6fe7d6ae-9630-4d26-a02c-3420516cf28f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fastai.data.core.DataLoaders at 0x79c5d1532da0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample -- first batch\n",
        "x,y = first(dls.train)\n",
        "print(\"x shape:\", x.shape)\n",
        "print(x)\n",
        "print()\n",
        "print(\"y shape:\", y.shape)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY9n-AvFBKqV",
        "outputId": "46a30c11-3609-4485-8865-73f77a1852b4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: torch.Size([10, 3])\n",
            "tensor([[ 0,  1,  2],\n",
            "        [ 1,  3,  1],\n",
            "        [ 4,  1,  5],\n",
            "        [ 1,  6,  1],\n",
            "        [ 7,  1,  8],\n",
            "        [ 1,  9,  1],\n",
            "        [10,  1, 11],\n",
            "        [ 1, 12,  1],\n",
            "        [13,  1, 14],\n",
            "        [ 1, 15,  1]])\n",
            "\n",
            "y shape: torch.Size([10])\n",
            "tensor([ 1,  4,  1,  7,  1, 10,  1, 13,  1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Predictions\n",
        "For this baseline model, we will always predict the most common token"
      ],
      "metadata": {
        "id": "Oe-ebRNMkDVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count the number of times each token in the vocab appears in the validation set\n",
        "n,counts = 0,torch.zeros(len(vocab))\n",
        "\n",
        "for x,y in dls.valid:\n",
        "  n += y.shape[0]\n",
        "  for i in range_of(vocab):\n",
        "    counts[i] += (y==i).long().sum()\n",
        "\n",
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA8FkIjIlDB1",
        "outputId": "3f168e82-2fe2-4718-95f3-61d178be2aee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([106., 637., 159., 107., 106., 159., 108., 106., 464., 442.,   6.,   7.,   6.,   6.,   7.,   6.,   6.,   7.,   6.,   6.,  64.,  63.,  63.,  64.,  63.,  63.,  66.,  66., 600., 638.])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the most common token\n",
        "idx = torch.argmax(counts)\n",
        "\n",
        "print(\"index of most common token:\", idx)\n",
        "print(\"most common token:\", vocab[idx.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FprsZkfkL20",
        "outputId": "355a121f-ae59-4ce0-e7f7-0981db4c6e8a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index of most common token: tensor(29)\n",
            "most common token: thousand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you always predict that token, how accurate will you be?\n",
        "print(\"number of counts for the most common token:\", counts[idx].item())\n",
        "print(\"total number of tokens in the corpus:\", n)\n",
        "print(\"accuracy, if always predict most common token:\", (counts[idx].item()/n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX9XlGgakLn6",
        "outputId": "ed39c271-3900-40f4-bf84-766d14df71d4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of counts for the most common token: 638.0\n",
            "total number of tokens in the corpus: 4207\n",
            "accuracy, if always predict most common token: 0.15165200855716662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 1: Unrolled representation\n",
        "Unrolled = the representation of an RNN before refactoring with a for loop"
      ],
      "metadata": {
        "id": "Kb_tg3uQnqrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model architecture\n",
        "- NN architecture:\n",
        "  - First linear layer inputs = first word's embedding\n",
        "  - Second linear layer inputs = second word's embedding + the first layer's output activations\n",
        "  - Third linear layer inputs = third word's embedding + the second layer's output activations\n",
        "  - Why? It takes the context of every word into account -- every word is interpreted in the information context of any words preceding it\n",
        "- Each of these three layers will use the same weight matrix\n",
        "  -  The way that one word impacts the activations from previous words should not change depending on the position of a word\n",
        "  - In other words, activation values will change as the data moves through the layers, but the layer weights themselves will not change from layer to layer\n",
        "  - Aka a layer does not learn one sequence position -- it must learn to handle all positions\n",
        "  - Since layer weights don't change, you can think of the sequential layers as \"the same layer\" repeated"
      ],
      "metadata": {
        "id": "a7bh0gZrc6FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "cvaXetTfpCYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall our vocab\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5fLR6GdpGDc",
        "outputId": "801a92c5-3c4b-4cad-b26b-29f33a9695e4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', '.', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen', 'nineteen', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'thousand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x -- first 3 words in a sequence\n",
        "# numbers = indices of the tokens in the vocab\n",
        "x,y = first(dls.train)\n",
        "print(\"first batch of x:\")\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKkWfPF9pFOi",
        "outputId": "6f9ac999-944a-4388-8044-7b31631002b3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first batch of x:\n",
            "tensor([[ 0,  1,  2],\n",
            "        [ 1,  3,  1],\n",
            "        [ 4,  1,  5],\n",
            "        [ 1,  6,  1],\n",
            "        [ 7,  1,  8],\n",
            "        [ 1,  9,  1],\n",
            "        [10,  1, 11],\n",
            "        [ 1, 12,  1],\n",
            "        [13,  1, 14],\n",
            "        [ 1, 15,  1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y -- 4th word in a sequence\n",
        "print(\"first batch of y:\")\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CAXFNtZpwC9",
        "outputId": "cb401db9-fb07-420e-92d1-095e4f46c888"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first batch of y:\n",
            "tensor([ 1,  4,  1,  7,  1, 10,  1, 13,  1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Get activations for the FIRST token in the sequence"
      ],
      "metadata": {
        "id": "FSM_E4mEz3wN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 1 (embedding layer)\n",
        "- Get hidden state (the embeddings for token 1)\n",
        "- Input --> hidden"
      ],
      "metadata": {
        "id": "J17eFPqYrSBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input\n",
        "- Indices of the first token from each sample in the batch\n",
        "- Shape = [batch size]\n",
        "  - batch size = the number of samples you have"
      ],
      "metadata": {
        "id": "8OLhx0l-IPHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input = the first COLUMN of x (first word of each sequences)\n",
        "input_tok1 = x[:,0]\n",
        "\n",
        "print(\"number of inputs:\", input_tok1.shape)\n",
        "print(\"numericalized input:\", input_tok1)\n",
        "print(\"corresponding tokens:\", vocab[input_tok1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb9rn_Hxq4E6",
        "outputId": "37f344e7-e3fd-4065-afcd-348882db457d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of inputs: torch.Size([10])\n",
            "numericalized input: tensor([ 0,  1,  4,  1,  7,  1, 10,  1, 13,  1])\n",
            "corresponding tokens: ['one', '.', 'four', '.', 'seven', '.', 'ten', '.', 'thirteen', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define embedding layer architecture\n",
        "- Shape = [vocab size, number of hidden dimensions]\n",
        "  - number of hidden dimensions = number of dimensions you want for the embeddings"
      ],
      "metadata": {
        "id": "4RQOHzBWI24L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose number of embedding dimensions\n",
        "n_hidden = 5\n",
        "print(\"number of hidden layers:\", n_hidden)\n",
        "\n",
        "# get vocab size\n",
        "vocab_sz = len(vocab)\n",
        "print(\"vocab size:\", vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM-xoFNGr3jp",
        "outputId": "c1c73eda-bc83-49ac-8228-1d3fc864642d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of hidden layers: 5\n",
            "vocab size: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create embedding layer architecture\n",
        "input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
        "input_to_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2omucazy0BcK",
        "outputId": "44561b7d-3bbf-4ab4-fc28-b7b4cfc3fadc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get hidden state\n",
        "- Turn the raw input into embeddings, using the embedding architecture just defined\n",
        "- Hidden state for the first word = embeddings for the first word\n",
        "  - These are randomly initialized for now\n",
        "  - Will be updated with SGD later\n",
        "- Shape = [batch size, number of hidden dimensions]"
      ],
      "metadata": {
        "id": "vJhCpbZfJMqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get hidden state (embeddings for the first word of each sequence)\n",
        "hidden_tok1_emb = input_to_hidden(input_tok1)\n",
        "print(\"hidden state -- embeddings for first word of each sequence\")\n",
        "print(hidden_tok1_emb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdaHq64osw8g",
        "outputId": "ee83d834-83d4-4a97-a7a4-d18111098782"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden state -- embeddings for first word of each sequence\n",
            "tensor([[ 0.3057, -0.7746,  0.0349,  0.3211,  1.5736],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [ 1.8113,  0.1606,  0.3672,  0.1754, -1.1845],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [ 0.2311,  0.0087, -0.1423,  0.1971, -1.1441],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [ 0.7281, -0.7106, -0.6021,  0.9604,  0.4048],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [-0.4502, -0.6788,  0.5743,  0.1877, -0.3576],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"batch size: \", len(x))\n",
        "print(\"number of hidden dimensions:\", n_hidden)\n",
        "print(\"hidden layer shape:\", hidden_tok1_emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy35kumFuI9Q",
        "outputId": "47f9ccf2-1e74-4772-a1ee-d19dd592ec0e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size:  10\n",
            "number of hidden dimensions: 5\n",
            "hidden layer shape: torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 2 (linear layer)\n",
        "- Matrix multiplication (hidden state * initialized linear params)\n",
        "- Hidden --> hidden"
      ],
      "metadata": {
        "id": "hrfuoMk9K1uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define linear layer architecture\n",
        "- Initializes the parameter matrices (weights and bias)\n",
        "- Weights shape = [number of hidden dimensions, number of hidden dimensions]\n",
        "- Bias shape = [number of hidden dimensions]"
      ],
      "metadata": {
        "id": "4uYH7NPYLI_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_to_hidden = nn.Linear(n_hidden, n_hidden)\n",
        "\n",
        "print('Structure:\\t',hidden_to_hidden)\n",
        "print('Weights shape:\\t',hidden_to_hidden.weight.shape)\n",
        "print('Bias shape:\\t',hidden_to_hidden.bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyCAB7ZrnnMv",
        "outputId": "d134726f-3520-46be-c7cb-16281ea082c5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure:\t Linear(in_features=5, out_features=5, bias=True)\n",
            "Weights shape:\t torch.Size([5, 5])\n",
            "Bias shape:\t torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Update the hidden state\n",
        "- Apply the initialized linear parameters (weights/bias) to the hidden state (from the embedding layer)\n",
        "- The linear function does matrix multiplication with the initialized hidden state (the first word from our x samples) and parameter matrices\n",
        "- Result = updated hidden state\n",
        "- Shape = [batch size, number of hidden dimensions]"
      ],
      "metadata": {
        "id": "vWtVmFQ9Ol1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_tok1_lin = hidden_to_hidden(hidden_tok1_emb)\n",
        "print(\"updated hidden state:\")\n",
        "print(hidden_tok1_lin)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKbPGK9YyZS9",
        "outputId": "69a7ed44-c2a3-40ad-abad-f3c338880f6d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated hidden state:\n",
            "tensor([[-0.6069, -1.0809,  0.2330,  0.1919, -0.4181],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.5230,  0.2812,  0.0594,  0.0501,  0.1410],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.3221, -0.1254,  0.0772,  0.1889,  0.6168],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.5450, -1.0688, -0.1006, -0.1317, -0.3340],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.4286, -0.4842,  0.0209,  0.6692,  0.7102],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape of updated hidden state:\", hidden_tok1_lin.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dd3xI8MyBY2",
        "outputId": "e370dc79-a88a-445d-f1ca-f88afaf08bac"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of updated hidden state: torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 3 (output layer)\n",
        "- Get activations by applying ReLu to the updated hidden state\n",
        "- ReLu means replace every negative number with 0\n",
        "- Shape = [batch size, number of hidden dimensions]"
      ],
      "metadata": {
        "id": "vK8USTN7Pcd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply relu to the (updated) hidden state\n",
        "activations_tok1 = F.relu(hidden_tok1_lin)\n",
        "activations_tok1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBANuabK2nod",
        "outputId": "d8eac47b-76ca-42f6-f04a-08e354e8e0db"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.2330, 0.1919, 0.0000],\n",
              "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
              "        [0.0000, 0.2812, 0.0594, 0.0501, 0.1410],\n",
              "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
              "        [0.0000, 0.0000, 0.0772, 0.1889, 0.6168],\n",
              "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
              "        [0.0000, 0.0000, 0.0209, 0.6692, 0.7102],\n",
              "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape of the activations:\", activations_tok1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W30JYNHaygSf",
        "outputId": "a920de74-50e8-42d5-e53a-4940b4dc8cc8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the activations: torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of what happened"
      ],
      "metadata": {
        "id": "efYIDbcKP9fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token 1 summary\n",
        "print('raw input for token 1\\n'.upper(), input_tok1)\n",
        "print()\n",
        "print('vocab for token 1\\n'.upper(), vocab[input_tok1])\n",
        "print()\n",
        "print('embedding structure applied --> hidden state\\n'.upper(), hidden_tok1_emb)\n",
        "print()\n",
        "print('linear function applied --> updated hidden state\\n'.upper(), hidden_tok1_lin)\n",
        "print()\n",
        "print('relu applied --> activations for token 1\\n'.upper(), activations_tok1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpsqTduuP-2B",
        "outputId": "c9217cba-0972-4f9c-8e4a-f7029b2ce0de"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW INPUT FOR TOKEN 1\n",
            " tensor([ 0,  1,  4,  1,  7,  1, 10,  1, 13,  1])\n",
            "\n",
            "VOCAB FOR TOKEN 1\n",
            " ['one', '.', 'four', '.', 'seven', '.', 'ten', '.', 'thirteen', '.']\n",
            "\n",
            "EMBEDDING STRUCTURE APPLIED --> HIDDEN STATE\n",
            " tensor([[ 0.3057, -0.7746,  0.0349,  0.3211,  1.5736],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [ 1.8113,  0.1606,  0.3672,  0.1754, -1.1845],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [ 0.2311,  0.0087, -0.1423,  0.1971, -1.1441],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [ 0.7281, -0.7106, -0.6021,  0.9604,  0.4048],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879],\n",
            "        [-0.4502, -0.6788,  0.5743,  0.1877, -0.3576],\n",
            "        [-0.8455,  1.3123,  0.6872, -1.2347, -0.4879]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "LINEAR FUNCTION APPLIED --> UPDATED HIDDEN STATE\n",
            " tensor([[-0.6069, -1.0809,  0.2330,  0.1919, -0.4181],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.5230,  0.2812,  0.0594,  0.0501,  0.1410],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.3221, -0.1254,  0.0772,  0.1889,  0.6168],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.5450, -1.0688, -0.1006, -0.1317, -0.3340],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290],\n",
            "        [-0.4286, -0.4842,  0.0209,  0.6692,  0.7102],\n",
            "        [-0.3987,  0.6445,  1.0581,  0.4980,  1.3290]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "RELU APPLIED --> ACTIVATIONS FOR TOKEN 1\n",
            " tensor([[0.0000, 0.0000, 0.2330, 0.1919, 0.0000],\n",
            "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
            "        [0.0000, 0.2812, 0.0594, 0.0501, 0.1410],\n",
            "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
            "        [0.0000, 0.0000, 0.0772, 0.1889, 0.6168],\n",
            "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290],\n",
            "        [0.0000, 0.0000, 0.0209, 0.6692, 0.7102],\n",
            "        [0.0000, 0.6445, 1.0581, 0.4980, 1.3290]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Get activations for the SECOND word in the sequence"
      ],
      "metadata": {
        "id": "jlIRxFWt24L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 1 (embedding layer)\n",
        "Update hidden state based on the embeddings of the SECOND token and the activations from the FIRST token"
      ],
      "metadata": {
        "id": "4LEqUfzaTpIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input\n",
        "Indices of the second token from each sample in the batch"
      ],
      "metadata": {
        "id": "rvEglpDg07F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input = the second COLUMN of x (the second word in each sequence)\n",
        "# shape = [batch size]\n",
        "input_tok2 = x[:,1]\n",
        "print(input_tok2.shape)\n",
        "print(input_tok2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2yKETt3TcSp",
        "outputId": "b19c60d6-eed2-4e7c-f3cb-b09c2179b087"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "tensor([ 1,  3,  1,  6,  1,  9,  1, 12,  1, 15])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get embeddings for SECOND token"
      ],
      "metadata": {
        "id": "gkZzCBrjWXTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the same embedding architecture\n",
        "input_to_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkB-SnVJ1Efu",
        "outputId": "caca42fd-0bf9-4c28-8c49-f8604a4f400a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get embeddings\n",
        "# shape = [batch size, number of hidden dimensions]\n",
        "hidden_tok2_emb = input_to_hidden(input_tok2)\n",
        "hidden_tok2_emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flR-SlMlTcQc",
        "outputId": "9140ca3a-9f1e-4080-fd61-9a9f1a22797b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Update hidden state with the output of the first token\n",
        "Activations from the FIRST token + embeddings for the SECOND token"
      ],
      "metadata": {
        "id": "xd8zuE7s1D3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# updated hidden state\n",
        "# shape = [batch size, number of hidden dimensions]\n",
        "hidden_tok2_updated = activations_tok1 + hidden_tok2_emb\n",
        "hidden_tok2_updated.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddRwzfhBWtKa",
        "outputId": "b3fd8858-7b96-4a3a-97aa-c9f832627cb2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 2 (linear layer)\n",
        "Update the hidden state with the parameters (weights/bias)"
      ],
      "metadata": {
        "id": "NOnspoR61m_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the same linear architecture (weights and biases)\n",
        "hidden_to_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XHPtF1T1oXK",
        "outputId": "742ecac7-c19a-4be4-d9f3-f65db699a8be"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=5, out_features=5, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply linear\n",
        "# shape = [batch size, number of hidden dimensions]\n",
        "hidden_tok2_lin = hidden_to_hidden(hidden_tok2_updated)\n",
        "print(hidden_tok2_lin.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psZ-w-dZ1t1Y",
        "outputId": "7b2cd940-febc-4e7b-d851-564f9dec8011"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 3 (output layer)\n",
        "Get activations for the second token"
      ],
      "metadata": {
        "id": "7UGoxzElXVx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply relu\n",
        "# shape = [batch size, number of hidden dimensions]\n",
        "activations_tok2 = F.relu(hidden_tok2_lin)\n",
        "print(activations_tok2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0sXerqsTcGN",
        "outputId": "94dffe82-0a4e-48cd-e20e-eecfac6b10ef"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Get activations for the THIRD word in the sequence"
      ],
      "metadata": {
        "id": "tB38B-xjXvgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 1 (embedding layer)\n",
        "Update hidden state based on the embeddings of the THIRD token and the activations from the SECOND token"
      ],
      "metadata": {
        "id": "UVE6hfDyYXS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input = the third of x\n",
        "input_tok3 = x[:,2]\n",
        "print(input_tok3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvrDHmY4XxhD",
        "outputId": "22bc1b61-af7b-4663-daaf-9e94544e60dc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get embeddings (using the same embedding architecture)\n",
        "hidden_tok3_emb = input_to_hidden(input_tok3)\n",
        "hidden_tok3_emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEfFbPkVYUm_",
        "outputId": "7a1ec473-d3a7-4921-f707-295f38658d29"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update hidden state (by including the activations from the second token)\n",
        "hidden_tok3_updated = activations_tok2 + hidden_tok3_emb\n",
        "hidden_tok3_updated.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yqleXKVYUkR",
        "outputId": "804861f6-3df6-4d6f-e96c-9e7a89c5370a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 2 (linear layer)\n",
        "Update the hidden state with parameters (weights / biases)"
      ],
      "metadata": {
        "id": "YDHAZHMAYn_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update hidden state (using the same linear architecture)\n",
        "hidden_tok3_lin = hidden_to_hidden(hidden_tok3_updated)\n",
        "print(hidden_tok3_lin.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrYrL2x53GRZ",
        "outputId": "c57aa5ce-2e0d-429e-db62-4f0c8aea9cb0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer 3 (output layer)\n",
        "Get activations for the third token"
      ],
      "metadata": {
        "id": "T0WA_HQS3Gie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply relu\n",
        "activations_tok3 = F.relu(hidden_tok3_lin)\n",
        "print(activations_tok3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2j8wdoXYUiL",
        "outputId": "c5ee4aba-2816-46f1-976a-c68ca5d44a8d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Get final outputs (activations)\n",
        "- Goal: Predict the next (4th) word in the sequence\n",
        "- Hidden --> output"
      ],
      "metadata": {
        "id": "UiUwQaNGZY_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall vocab shape\n",
        "print(\"number of tokens in the vocab:\", vocab_sz)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ejSxJd-6fF4",
        "outputId": "11b3ed2e-d743-4823-fdb7-928c36e075f4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of tokens in the vocab: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall y\n",
        "print(\"y shape:\", y.shape)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG0Y5MdXaQSv",
        "outputId": "578423c2-0476-4415-90fe-67d87178df89"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y shape: torch.Size([10])\n",
            "tensor([ 1,  4,  1,  7,  1, 10,  1, 13,  1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define linear architecture\n",
        "- Matrix multiplication (hidden state * initialized linear params)\n",
        "- Initializes the parameter matrices (weights and bias)\n",
        "- Weights shape = [vocab size, number of hidden dimensions]\n",
        "- Bias shape = [vocab size]"
      ],
      "metadata": {
        "id": "JmU0AhUI7K4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the hidden to output linear layer\n",
        "hidden_to_output = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "print('Structure:\\t',hidden_to_output)\n",
        "print('Weights shape:\\t',hidden_to_output.weight.shape)\n",
        "print('Bias shape:\\t',hidden_to_output.bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7O4We39Yvwp",
        "outputId": "b346577c-e2a4-49ec-b7d9-f6912a87770e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure:\t Linear(in_features=5, out_features=30, bias=True)\n",
            "Weights shape:\t torch.Size([30, 5])\n",
            "Bias shape:\t torch.Size([30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the outputs\n",
        "- Apply the initialized linear parameters (from the architecture above) to the hidden state we created above (aka the activations of the third token)\n",
        "- The linear function does matrix multiplication with the hidden state and the initialized parameter matrices\n",
        "- Result = predictions of the next word in the sequence\n",
        "- Shape = [batch size, vocab size]\n",
        "  - In other words, for each sequence (1 sample of the batch), get an activation for EVERY word in the vocab\n",
        "  - Vocab size = number of \"classes\""
      ],
      "metadata": {
        "id": "O5y_ER2u7zj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply these initialized linear parameters to the hidden state\n",
        "outputs = hidden_to_output(activations_tok3)"
      ],
      "metadata": {
        "id": "T3HN0NYfYvsA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note the shape is the batch size x vocab size\n",
        "print(outputs.shape)\n",
        "# outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcewop8K8Z0w",
        "outputId": "9d575da4-4b7f-4f23-98fa-574cbb6ce090"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same Model -- Class Format\n",
        "Unrolled representation"
      ],
      "metadata": {
        "id": "A1_TGNHvcP2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 1\n",
        "Every step is completely written out for each token in the x sequence"
      ],
      "metadata": {
        "id": "jGBF0C_zZXm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel1(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    \"\"\"\n",
        "    Initialize parameters\n",
        "    \"\"\"\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden) # initialize embeddings\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden) # initialize parameters (weights/bias) for the hidden linear layers\n",
        "    self.h_o = nn.Linear(n_hidden,vocab_sz) # initialize parameters for the final layer (getting output)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Calculate the activations for each of the 3 layers\n",
        "    i_h (input to hidden) = embedding layer\n",
        "    h_h (hidden to hidden) = linear layer that creates the activations for the next word\n",
        "    h_o (hidden to output) = final linear layer that predicts the fourth word\n",
        "    \"\"\"\n",
        "    ### Get activations for the FIRST word in the sequence ###\n",
        "    # Layer 1 (embedding layer): Get hidden state (the embeddings for word 1)\n",
        "    h = self.i_h(x[:,0]) # first word = x[:,0]\n",
        "\n",
        "    # Layer 2 (linear layer): Matrix multiplication (hidden state * initialized linear params)\n",
        "    h = self.h_h(h)\n",
        "\n",
        "    # Layer 3 (output layer): Get activations with ReLu (replace every negative number with 0)\n",
        "    h = F.relu(h)\n",
        "\n",
        "    ### Get activations for the SECOND word in the sequence ###\n",
        "    # Layer 1 (embedding layer): Update hidden state (embeddings for word 2 + activations from word 1)\n",
        "    h = h + self.i_h(x[:,1]) # second word = x[:,1]\n",
        "\n",
        "    # Layer 2 (linear layer): Matrix multiplication (hidden state * initialized linear params)\n",
        "    h = self.h_h(h)\n",
        "\n",
        "    # Layer 3 (output layer): Get activations with ReLu\n",
        "    h = F.relu(h)\n",
        "\n",
        "    ### Get activations for the THIRD word in the sequence ###\n",
        "    # Layer 1 (embedding layer): Update hidden state (the embeddings for word 3 + activations from second layer)\n",
        "    h = h + self.i_h(x[:,2]) # third word = x[:,2]\n",
        "\n",
        "    # Layer 2 (linear layer): Matrix multiplication\n",
        "    h = self.h_h(h)\n",
        "\n",
        "    # Layer 3 (output layer): Get activations with ReLu\n",
        "    h = F.relu(h)\n",
        "\n",
        "    ### Get Final Activations -- predicting 4th word in the sequence ###\n",
        "    # Final linear layer: Matrix multiplication (hidden state * initialized linear params)\n",
        "    # Note: often, softmax is applied to these outputs, but not in this function\n",
        "    out = self.h_o(h)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "aQEKFhgZLkmi"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 2\n",
        "- Same model as above, but written in a more RNN-y format\n",
        "  - A neural network that is defined using a loop is called a recurrent neural network (RNN)\n",
        "  - An RNN is not a complicated new architecture, but simply a refactoring of a multilayer neural network using a for loop\n",
        "- Hidden state = the activations that are updated at each step of a recurrent neural network"
      ],
      "metadata": {
        "id": "9cjn0C2wZlLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel2(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    \"\"\"\n",
        "    Initialize parameters\n",
        "    \"\"\"\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Calculate the activations\n",
        "    \"\"\"\n",
        "    ### Initialize the hidden state ###\n",
        "    # h will be (re)set to 0 for EVERY sample of x (where x = a sequence of words)\n",
        "    # this is not ideal -- we will fix this later with `stateful`\n",
        "    h = 0\n",
        "\n",
        "    ### Get/update the hidden state ###\n",
        "    # this is done for each of tokens in the sequence\n",
        "    # note \"3\" can be replaced with any length\n",
        "    for i in range(3):\n",
        "      h = h + self.i_h(x[:,i]) # get embeddings and update hidden state\n",
        "      h = F.relu(self.h_h(h)) # linear layer, followed by activation function (ReLU)\n",
        "\n",
        "    ### Get final activations (predictions) ###\n",
        "    out = self.h_o(h)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "YFf0U6oAZVEc"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model\n",
        "- Plug in batches of x data into model architecture\n",
        "- Define the number of hidden dimensions you want\n",
        "- Get prediced y values (using the model)\n",
        "- Get loss / metrics\n",
        "- Optimize with SGD\n",
        "- Do this over multiple epochs"
      ],
      "metadata": {
        "id": "tbOv36h-grSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataloaders using larger batch size\n",
        "bs = 64\n",
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, shuffle=False)"
      ],
      "metadata": {
        "id": "rw1oy1erkOA1"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make modeling choices\n",
        "n_hidden = 64\n",
        "model = LMModel1(len(vocab), n_hidden)\n",
        "\n",
        "# train\n",
        "learn = Learner(\n",
        "  dls,\n",
        "  model,\n",
        "  loss_func=F.cross_entropy,\n",
        "  metrics=accuracy\n",
        ")\n",
        "\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "id": "0bAMOQUbc_lT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "2c904d2a-d1f0-4766-dc4f-fa58b1142809"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.784861</td>\n",
              "      <td>2.031090</td>\n",
              "      <td>0.474210</td>\n",
              "      <td>00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.403965</td>\n",
              "      <td>1.836548</td>\n",
              "      <td>0.468743</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.395346</td>\n",
              "      <td>1.708262</td>\n",
              "      <td>0.492750</td>\n",
              "      <td>00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.371430</td>\n",
              "      <td>1.670374</td>\n",
              "      <td>0.494176</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show that the results are (roughly) the same in version 2\n",
        "model = LMModel2(len(vocab), n_hidden)\n",
        "learn = Learner(dls, model, loss_func=F.cross_entropy, metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "tNUMB8hOaqYl",
        "outputId": "8ca76d91-c8cd-4fba-d0f4-9abc0fa3a2bc"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.813884</td>\n",
              "      <td>1.904099</td>\n",
              "      <td>0.465652</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.391428</td>\n",
              "      <td>1.797721</td>\n",
              "      <td>0.468505</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.416528</td>\n",
              "      <td>1.664593</td>\n",
              "      <td>0.493701</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.368125</td>\n",
              "      <td>1.710180</td>\n",
              "      <td>0.420490</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 2: Stateful version of the model\n",
        "- The previous models were not stateful\n",
        "  - The hidden state was initialized to zero for every new input sequence (aka for token 1, token 2, and token 3 in our 3-token sequence)\n",
        "  - That is problematic because the sample sequences will be read in order by the model, exposing the model to long stretches of the original sequence -- by initializing the hidden state to 0 each time, you are throwing away all the information it has seen before\n",
        "  - It also means that the model doesn't actually know where it is in the overall counting sequence\n",
        "- Stateful models\n",
        "  - The model remembers its activations between different calls to `forward` (which represent its use for different samples in the batch)\n",
        "  - Aka it remembers activations from one x sample to the next"
      ],
      "metadata": {
        "id": "JqIuKn3DdALS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "- Need to make sure the samples are going to be seen in a certain order\n",
        "- If the first line of the first batch is our `dset[0]` then the second batch should have `dset[1]` as the first line, so that the model sees the text flowing -- refer back to **_08_nlp_basics.ipynb_** for details on this"
      ],
      "metadata": {
        "id": "Td_hqD0_juCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous Version"
      ],
      "metadata": {
        "id": "ickL5K4Kibxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bs = 64\n",
        "x,y = first(dls.train)\n",
        "print(x[:5])\n",
        "print(y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei0O_hcspCsh",
        "outputId": "d9d7563f-09c5-41d6-fa82-c76811cdac10"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [1, 3, 1],\n",
            "        [4, 1, 5],\n",
            "        [1, 6, 1],\n",
            "        [7, 1, 8]])\n",
            "tensor([1, 4, 1, 7, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rearranged Version"
      ],
      "metadata": {
        "id": "Ep2Ddf9pie4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Divide the dataset into m groups (of size `batch_size`)"
      ],
      "metadata": {
        "id": "q_gX8XvSmalt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_chunks(ds, bs):\n",
        "  \"\"\"\n",
        "  Group the dataset into groups with a special order\n",
        "  \"\"\"\n",
        "  m = len(ds) // bs # divide the dataset into m groups (of size batch_size)\n",
        "  new_ds = L() # initialize new dataset\n",
        "\n",
        "  # rearrange datasets so...\n",
        "    # batch 1 will be (0, m, 2m, ..., (bs-1)m)\n",
        "    # batch 2 will be (1, m+1, 2m+1, ..., (bs-1)m+1)\n",
        "    # etc.\n",
        "  for i in range(m):\n",
        "    new_ds += L(ds[i + m*j] for j in range(bs))\n",
        "\n",
        "  return new_ds"
      ],
      "metadata": {
        "id": "9Oqz81fQpCqH"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataloaders"
      ],
      "metadata": {
        "id": "5pZ_XT1Univ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64\n",
        "cut = int(len(seqs) * 0.8) # where to split into train/valid sets\n",
        "\n",
        "grouped_dls = DataLoaders.from_dsets(\n",
        "  group_chunks(seqs[:cut], bs), # train set\n",
        "  group_chunks(seqs[cut:], bs), # valid set\n",
        "  bs=bs, # batch size\n",
        "  drop_last = True, # drop the last batch (which will not have the shape of bs)\n",
        "  shuffle=False # make sure the texts are read in order\n",
        ")"
      ],
      "metadata": {
        "id": "mIFzZfqfjuqT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### View data\n",
        "- At each epoch, the model will see a chunk of contiguous text of size 3*m (since each text is of size 3) on each line of the batch"
      ],
      "metadata": {
        "id": "7ggAF6v_n71q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first batch\n",
        "x,y = first(grouped_dls.train)\n",
        "print(x.shape, y.shape)\n",
        "x[:5], y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRv_XmpyqBQx",
        "outputId": "1e4e05a4-e28c-46a3-b8e3-1d494613b624"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3]) torch.Size([64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  1,  2],\n",
              "         [11,  1,  2],\n",
              "         [25,  7,  1],\n",
              "         [ 5,  1,  5],\n",
              "         [28, 12,  1]]),\n",
              " tensor([ 1, 28,  3, 28,  7]))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "- Stateful\n",
        "  - The model remembers the activations between calls to `forward` (call `forward` for each new x sample in the batch)\n",
        "  - Only resets the hidden state at the beginning of each epoch/validation set\n",
        "  - The model will have the same activations for whatever sequence length we pick, because the hidden state will remember the last activation from the previous batch\n",
        "- Backpropogation Through Time (BPTT)\n",
        "  - Only calculate gradients on sequence length tokens in the past, instead of the whole stream\n",
        "  - \"Detach\" the not-needed history of gradients\n",
        "  - Treats a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way\n",
        "  - To avoid running out of memory and time, we usually use truncated BPTT, which \"detaches\" the history of computation steps in the hidden state every few time steps"
      ],
      "metadata": {
        "id": "0zDDnXzodDAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel3(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    \"\"\"\n",
        "    Initialize the parameters for each layer\n",
        "    Initialize the hidden state (h)\n",
        "    \"\"\"\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "    # initializing the hidden state here allows the model to be stateful\n",
        "    self.h = 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Get activations for each token in the x sequence (of length 3)\n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "      self.h = self.h + self.i_h(x[:,i]) # get embeddings and update hidden state\n",
        "      self.h = F.relu(self.h_h(self.h)) # linear layer and relu layer\n",
        "\n",
        "    out = self.h_o(self.h) # get outputs -- predictions for the 4th word\n",
        "    self.h = self.h.detach() # detach gradient -- remove its history (for memory size purposes)\n",
        "    return out\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the hidden state at the beginning of each epoch and before each validation phase\n",
        "    Helps make sure we start with a clean state before reading continuous chunks of text\n",
        "    This is called in the learner\n",
        "    \"\"\"\n",
        "    self.h = 0\n"
      ],
      "metadata": {
        "id": "2UdJfXYHWJRc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "mDuctfGFoESq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train and fit\n",
        "learn = Learner(\n",
        "  grouped_dls,\n",
        "  LMModel3(len(vocab), 64), # vocab and n_hidden\n",
        "  loss_func=F.cross_entropy,\n",
        "  metrics=accuracy,\n",
        "  cbs=ModelResetter # reset the hidden state of the model at the beginning of each epoch and before each validation phase\n",
        ")\n",
        "\n",
        "learn.fit_one_cycle(10, 3e-3)"
      ],
      "metadata": {
        "id": "79y53VFsjf1Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "162bf2f5-dbc4-41fa-b079-d1842059a451"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.682869</td>\n",
              "      <td>1.876281</td>\n",
              "      <td>0.409135</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.245189</td>\n",
              "      <td>1.751637</td>\n",
              "      <td>0.449519</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.095319</td>\n",
              "      <td>1.587953</td>\n",
              "      <td>0.503846</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.022529</td>\n",
              "      <td>1.643607</td>\n",
              "      <td>0.552885</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.992957</td>\n",
              "      <td>1.651556</td>\n",
              "      <td>0.539423</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.929095</td>\n",
              "      <td>1.819821</td>\n",
              "      <td>0.547356</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.877613</td>\n",
              "      <td>1.850660</td>\n",
              "      <td>0.563702</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.832252</td>\n",
              "      <td>1.670566</td>\n",
              "      <td>0.598558</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.792325</td>\n",
              "      <td>1.787299</td>\n",
              "      <td>0.599279</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.785346</td>\n",
              "      <td>1.722843</td>\n",
              "      <td>0.603365</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 3: Predicting the Next Word after Each Word\n",
        "- Input = word\n",
        "- Output = next word\n",
        "- This approach gives more signal -- it uses the intermediate predictions to predict the second and third words, in addition to the 4th word\n",
        "- Very deep network -- not great because can result in very large / very small gradients (will deal with this later)"
      ],
      "metadata": {
        "id": "tATOdIwNeZce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "apBXqES_sa0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous\n",
        "Predict 1 word after every 3 words"
      ],
      "metadata": {
        "id": "tmNkxZxapCI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = first(dls.train)\n",
        "\n",
        "print(\"X \\t\\t\\t Y\")\n",
        "for i in range(len(x[:10])):\n",
        "  print(x[i],\"\\t\", y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cddyNjyutHQm",
        "outputId": "7926debd-1044-4889-af05-655ace447183"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X \t\t\t Y\n",
            "tensor([0, 1, 2]) \t tensor(1)\n",
            "tensor([1, 3, 1]) \t tensor(4)\n",
            "tensor([4, 1, 5]) \t tensor(1)\n",
            "tensor([1, 6, 1]) \t tensor(7)\n",
            "tensor([7, 1, 8]) \t tensor(1)\n",
            "tensor([1, 9, 1]) \t tensor(10)\n",
            "tensor([10,  1, 11]) \t tensor(1)\n",
            "tensor([ 1, 12,  1]) \t tensor(13)\n",
            "tensor([13,  1, 14]) \t tensor(1)\n",
            "tensor([ 1, 15,  1]) \t tensor(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now\n",
        "- After every word, predict the next word\n",
        "- For each sequence in the sample, there are two lists of the same size\n",
        "  - The first list is x\n",
        "  - The second list is y\n",
        "  - y is offset by one element from x"
      ],
      "metadata": {
        "id": "amFnoMZ9pU-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make choices\n",
        "sl = 4 # sequence length (number of tokens in the sequence)\n",
        "bs = 10 # batch size (number of samples)"
      ],
      "metadata": {
        "id": "D1D6IAWItL8g"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create new sequences\n",
        "new_seqs = L(\n",
        "  (\n",
        "    tensor(nums[i:i+sl]), # x\n",
        "    tensor(nums[i+1:i+sl+1]) # y\n",
        "  )\n",
        "  for i in range(0,len(nums)-sl-1,sl) # sl tells you how long the sequence is\n",
        ")\n",
        "\n",
        "# first set of x and y\n",
        "new_seqs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKxqcLeguG5f",
        "outputId": "1d51f804-f980-4427-b4e3-507b26866f4d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 1]), tensor([1, 2, 1, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# explain more clearly\n",
        "for i in range(3):\n",
        "  x = new_seqs[i][0]\n",
        "  y = new_seqs[i][1]\n",
        "\n",
        "  x_vocab = [L(vocab[o] for o in new_seqs[i][0])]\n",
        "  y_vocab = [L(vocab[o] for o in new_seqs[i][1])]\n",
        "\n",
        "  print(\"SAMPLE\", i+1)\n",
        "  print(f\"X (raw input and vocab):\\t{x}\\t{x_vocab}\")\n",
        "  print(f\"Y (raw input and vocab):\\t{y}\\t{y_vocab}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1C-JHZCt6e1",
        "outputId": "91e5d053-b1c2-441f-f312-b23554f58149"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAMPLE 1\n",
            "X (raw input and vocab):\ttensor([0, 1, 2, 1])\t[['one', '.', 'two', '.']]\n",
            "Y (raw input and vocab):\ttensor([1, 2, 1, 3])\t[['.', 'two', '.', 'three']]\n",
            "\n",
            "SAMPLE 2\n",
            "X (raw input and vocab):\ttensor([3, 1, 4, 1])\t[['three', '.', 'four', '.']]\n",
            "Y (raw input and vocab):\ttensor([1, 4, 1, 5])\t[['.', 'four', '.', 'five']]\n",
            "\n",
            "SAMPLE 3\n",
            "X (raw input and vocab):\ttensor([5, 1, 6, 1])\t[['five', '.', 'six', '.']]\n",
            "Y (raw input and vocab):\ttensor([1, 6, 1, 7])\t[['.', 'six', '.', 'seven']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaders\n",
        "cut = int(len(new_seqs) * 0.8) # train / valid split\n",
        "\n",
        "new_dls = DataLoaders.from_dsets(\n",
        "  group_chunks(new_seqs[:cut], bs), # at each epoch, the model will see a chunk of contiguous text of size sl*m on each line of the batch\n",
        "  group_chunks(new_seqs[cut:], bs),\n",
        "  bs=bs, drop_last=True, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "88-yaBkMvKfw"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "x,y = first(new_dls.train)\n",
        "\n",
        "print(\"X \\t\\t\\t\\t\\t Y\")\n",
        "for i in range(len(x)):\n",
        "  print(x[i], \"\\t\", y[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWYpd8IrwjpZ",
        "outputId": "ecb45536-bf8b-4102-ebae-9c58c9e6d0bd"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X \t\t\t\t\t Y\n",
            "tensor([0, 1, 2, 1]) \t tensor([1, 2, 1, 3])\n",
            "tensor([ 1,  0, 29,  0]) \t tensor([ 0, 29,  0, 28])\n",
            "tensor([28, 24,  2,  1]) \t tensor([24,  2,  1,  0])\n",
            "tensor([28, 22,  4,  1]) \t tensor([22,  4,  1,  2])\n",
            "tensor([28, 20,  6,  1]) \t tensor([20,  6,  1,  3])\n",
            "tensor([ 6,  1,  4, 29]) \t tensor([ 1,  4, 29,  2])\n",
            "tensor([ 8,  1,  4, 29]) \t tensor([ 1,  4, 29,  9])\n",
            "tensor([ 1,  5, 29,  7]) \t tensor([ 5, 29,  7, 28])\n",
            "tensor([ 1,  6, 29,  5]) \t tensor([ 6, 29,  5, 28])\n",
            "tensor([28,  2,  1,  7]) \t tensor([ 2,  1,  7, 29])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get y preds using an RNN model architecture\n",
        "- Stateful\n",
        "- Outputs a prediction after every word (rather than just at the end of a sequence)"
      ],
      "metadata": {
        "id": "cNjssco2zBfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. Initialize layer structures"
      ],
      "metadata": {
        "id": "I2XZB6V2zKsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding layer (input --> hidden)\n",
        "- Creates the embedding structure\n",
        "- vocab_sz = number of tokens in the vocab\n",
        "- n_hidden = number of hidden dimensions (for the embeddings)"
      ],
      "metadata": {
        "id": "5Vp8FiTpx5lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sz = len(vocab)\n",
        "n_hidden = 5\n",
        "\n",
        "input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
        "input_to_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo-zyTdazDAU",
        "outputId": "4cfdd0c5-259a-4314-bce0-06af5cad7b01"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear (hidden) layer (hidden --> hidden)\n",
        "- Initializes the parameter matrices (weights/bias)\n",
        "- Creates the activations for the next token in the sequence"
      ],
      "metadata": {
        "id": "86k0nad8yG8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_to_hidden = nn.Linear(n_hidden, n_hidden)\n",
        "\n",
        "print('Structure:\\t', hidden_to_hidden)\n",
        "print('Weights shape:\\t', hidden_to_hidden.weight.shape)\n",
        "print('Bias shape:\\t', hidden_to_hidden.bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxr6krY6zDWY",
        "outputId": "64939ed5-c02c-4624-de9f-f8b41a527ae1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure:\t Linear(in_features=5, out_features=5, bias=True)\n",
            "Weights shape:\t torch.Size([5, 5])\n",
            "Bias shape:\t torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear (output) layer (hidden --> output)\n",
        "- Outputs predictions for the next token in the sequence by giving each token in the vocab a probability\n"
      ],
      "metadata": {
        "id": "DIoj8qSnyjsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_to_output = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "print('Structure:\\t',hidden_to_output)\n",
        "print('Weights shape:\\t',hidden_to_output.weight.shape)\n",
        "print('Bias shape:\\t',hidden_to_output.bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8j39orAzDUU",
        "outputId": "bb54357b-e1df-492b-ebc0-50e2887dc099"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure:\t Linear(in_features=5, out_features=30, bias=True)\n",
            "Weights shape:\t torch.Size([30, 5])\n",
            "Bias shape:\t torch.Size([30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize hidden state"
      ],
      "metadata": {
        "id": "ZZ4JnTr_y_uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = 0"
      ],
      "metadata": {
        "id": "iCqWAPKuGt_3"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. First token (of 4-token sequence)"
      ],
      "metadata": {
        "id": "1fniAC3Cz9OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw input = the first COLUMN of x\n",
        "input = x[:,0]\n",
        "print(input.shape)\n",
        "input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCLuK0vuzDR8",
        "outputId": "6fcdf233-1b64-483f-b847-b9153e5b1702"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1, 28, 28, 28,  6,  8,  1,  1, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get embeddings\n",
        "emb = input_to_hidden(input)\n",
        "print(emb.shape)\n",
        "emb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IOa-tg3zDPm",
        "outputId": "3fb4b19f-dc19-414f-c730-ca42cf1365d5"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0541,  1.4377, -0.6071, -0.6359, -1.4333],\n",
              "        [-0.3883,  0.5718, -0.7854, -0.2312,  0.0647],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957],\n",
              "        [ 0.2785,  2.4130,  0.4558, -0.9273, -0.4406],\n",
              "        [-0.2804,  0.3452,  0.5821,  0.0870,  0.7165],\n",
              "        [-0.3883,  0.5718, -0.7854, -0.2312,  0.0647],\n",
              "        [-0.3883,  0.5718, -0.7854, -0.2312,  0.0647],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update hidden state with embeddings\n",
        "# since hidden stat was initialized to 0, here, it's the same as the embeddings\n",
        "hidden = hidden + emb\n",
        "print(hidden.shape)\n",
        "hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGG_zsHLG7UG",
        "outputId": "08bd378d-e567-4ee7-fe6b-2ed5879496f0"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0541,  1.4377, -0.6071, -0.6359, -1.4333],\n",
              "        [-0.3883,  0.5718, -0.7854, -0.2312,  0.0647],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957],\n",
              "        [ 0.2785,  2.4130,  0.4558, -0.9273, -0.4406],\n",
              "        [-0.2804,  0.3452,  0.5821,  0.0870,  0.7165],\n",
              "        [-0.3883,  0.5718, -0.7854, -0.2312,  0.0647],\n",
              "        [-0.3883,  0.5718, -0.7854, -0.2312,  0.0647],\n",
              "        [-1.6021, -1.0879, -0.6479, -0.6746, -0.3957]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply linear layer\n",
        "hidden = hidden_to_hidden(hidden)\n",
        "print(hidden.shape)\n",
        "hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp0q2S8bzDNT",
        "outputId": "3c33c4a1-6d06-45c2-f268-9b8d498afc1f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7006e-01, -1.5248e-01,  2.4186e-01, -1.4339e-01, -8.7209e-01],\n",
              "        [ 5.9632e-01, -1.9563e-01, -1.5672e-01,  3.2954e-01, -3.3494e-01],\n",
              "        [ 1.0002e+00, -9.9780e-01,  4.7206e-01,  3.8699e-01,  6.9360e-04],\n",
              "        [ 1.0002e+00, -9.9780e-01,  4.7206e-01,  3.8699e-01,  6.9360e-04],\n",
              "        [ 1.0002e+00, -9.9780e-01,  4.7206e-01,  3.8699e-01,  6.9360e-04],\n",
              "        [-5.5977e-02,  9.2950e-01,  3.3538e-01, -5.3880e-01, -1.3467e+00],\n",
              "        [ 6.4640e-01,  4.8039e-01,  2.0160e-01,  1.3311e-01, -1.2474e-01],\n",
              "        [ 5.9632e-01, -1.9563e-01, -1.5672e-01,  3.2954e-01, -3.3494e-01],\n",
              "        [ 5.9632e-01, -1.9563e-01, -1.5672e-01,  3.2954e-01, -3.3494e-01],\n",
              "        [ 1.0002e+00, -9.9780e-01,  4.7206e-01,  3.8699e-01,  6.9360e-04]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply relu\n",
        "hidden = F.relu(hidden)\n",
        "print(hidden.shape)\n",
        "hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-VpeSnDzDFR",
        "outputId": "055304fc-d7d5-4bb4-c154-43032fc9e320"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000e+00, 0.0000e+00, 2.4186e-01, 0.0000e+00, 0.0000e+00],\n",
              "        [5.9632e-01, 0.0000e+00, 0.0000e+00, 3.2954e-01, 0.0000e+00],\n",
              "        [1.0002e+00, 0.0000e+00, 4.7206e-01, 3.8699e-01, 6.9360e-04],\n",
              "        [1.0002e+00, 0.0000e+00, 4.7206e-01, 3.8699e-01, 6.9360e-04],\n",
              "        [1.0002e+00, 0.0000e+00, 4.7206e-01, 3.8699e-01, 6.9360e-04],\n",
              "        [0.0000e+00, 9.2950e-01, 3.3538e-01, 0.0000e+00, 0.0000e+00],\n",
              "        [6.4640e-01, 4.8039e-01, 2.0160e-01, 1.3311e-01, 0.0000e+00],\n",
              "        [5.9632e-01, 0.0000e+00, 0.0000e+00, 3.2954e-01, 0.0000e+00],\n",
              "        [5.9632e-01, 0.0000e+00, 0.0000e+00, 3.2954e-01, 0.0000e+00],\n",
              "        [1.0002e+00, 0.0000e+00, 4.7206e-01, 3.8699e-01, 6.9360e-04]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get outputs!\n",
        "# one \"row\" for each sample in the batch; one \"column\" for each token in vocab\n",
        "outputs = hidden_to_output(hidden)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS7us7XTzkSh",
        "outputId": "60005387-278b-4ecb-bd0a-baa3a3806f13"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save outputs to list (will save an output for every token in the sequence)\n",
        "outs = []\n",
        "outs.append(outputs)\n",
        "len(outs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7dbW9FV1eVC",
        "outputId": "93115d76-e7d1-47ac-9c54-2fea9c634909"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Second token"
      ],
      "metadata": {
        "id": "ckRA9b9K0ixd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw input from SECOND column of x\n",
        "input = x[:,1]\n",
        "print(input.shape)\n",
        "\n",
        "# get embeddings\n",
        "emb = input_to_hidden(input)\n",
        "print(emb.shape)\n",
        "\n",
        "# update hidden state -- activations from first layer + embeddings of second column\n",
        "hidden = hidden + emb\n",
        "print(hidden.shape)\n",
        "\n",
        "# apply linear function\n",
        "hidden = hidden_to_hidden(hidden)\n",
        "print(hidden.shape)\n",
        "\n",
        "# apply relu\n",
        "hidden = F.relu(hidden)\n",
        "print(hidden.shape)\n",
        "print()\n",
        "\n",
        "# get outputs\n",
        "outputs = hidden_to_output(hidden)\n",
        "\n",
        "# save to output list\n",
        "outs.append(outputs)\n",
        "print(len(outs))\n",
        "print(outs[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TSFXyyK0jis",
        "outputId": "0cbe79f0-b99e-43e8-d52e-c162839998ac"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "torch.Size([10, 5])\n",
            "torch.Size([10, 5])\n",
            "torch.Size([10, 5])\n",
            "torch.Size([10, 5])\n",
            "\n",
            "2\n",
            "torch.Size([10, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps 3, 4, 5. Repeat for tokens 2, 3, and 4 (in the 4-token sequence)"
      ],
      "metadata": {
        "id": "h--zEPF91Bvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### TOKEN 2 ###\n",
        "# raw input from SECOND column of x\n",
        "input = x[:,1]\n",
        "\n",
        "# get embeddings\n",
        "emb = input_to_hidden(input)\n",
        "\n",
        "# update hidden state -- activations from first layer + embeddings of second column\n",
        "hidden = hidden + emb\n",
        "\n",
        "# apply linear function\n",
        "hidden = hidden_to_hidden(hidden)\n",
        "\n",
        "# apply relu\n",
        "hidden = F.relu(hidden)\n",
        "\n",
        "# get outputs\n",
        "outputs = hidden_to_output(hidden)\n",
        "outs.append(outputs)\n",
        "\n",
        "# save to output list\n",
        "len(outs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdLCfkZsrF_F",
        "outputId": "696ea4cb-adda-4436-ad54-b799311830bf"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKEN 3\n",
        "input = x[:,2] # raw input from THIRD column of x\n",
        "emb = input_to_hidden(input)\n",
        "hidden = hidden + emb # update hidden state -- activations from second layer + embeddings of third column\n",
        "hidden = hidden_to_hidden(hidden)\n",
        "hidden = F.relu(hidden)\n",
        "outputs = hidden_to_output(hidden)\n",
        "outs.append(outputs)\n",
        "len(outs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uITbDgG_qHl8",
        "outputId": "b9f27be3-59f2-4793-dc07-0f222cae2c5f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKEN 4\n",
        "input = x[:,3] # raw input from FOURTH column of x\n",
        "emb = input_to_hidden(input)\n",
        "hidden = hidden + emb # update hidden state -- activations from third layer + embeddings of fourth column\n",
        "hidden = hidden_to_hidden(hidden)\n",
        "hidden = F.relu(hidden)\n",
        "outputs = hidden_to_output(hidden)\n",
        "outs.append(outputs)\n",
        "len(outs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8SqbBDqqHY_",
        "outputId": "e29ed1ce-7bb6-4ce6-85c8-caa04d43f00a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat Steps 2-5 in a more efficient way\n",
        "- Loop for all tokens in the sequence\n",
        "- Recall, `sl` = sequence length"
      ],
      "metadata": {
        "id": "J4SDCphwqIAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_all = range(sl)\n",
        "outs = []\n",
        "\n",
        "for i in tokens_all:\n",
        "  print(\"TOKEN\", i+1)\n",
        "\n",
        "  input = x[:,i]\n",
        "  print(\"input shape:\", input.shape)\n",
        "\n",
        "  emb = input_to_hidden(input)\n",
        "  print(\"embedding shape:\", emb.shape)\n",
        "\n",
        "  hidden = hidden + emb\n",
        "  print(\"hidden shape:\", hidden.shape)\n",
        "\n",
        "  hidden = hidden_to_hidden(hidden)\n",
        "  print(\"hidden shape after linear:\", hidden.shape)\n",
        "\n",
        "  hidden = F.relu(hidden)\n",
        "  print(\"hidden shape after relu:\", hidden.shape)\n",
        "\n",
        "  outputs = hidden_to_output(hidden)\n",
        "  print(\"outputs shape:\", outputs.shape)\n",
        "\n",
        "  outs.append(outputs)\n",
        "  print(\"----\\n\")\n",
        "\n",
        "print(\"total number of outputs:\", len(outs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pI6ck2-1FMd",
        "outputId": "432f63e0-25a7-4ee7-9265-1d5481f2594e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKEN 1\n",
            "input shape: torch.Size([10])\n",
            "embedding shape: torch.Size([10, 5])\n",
            "hidden shape: torch.Size([10, 5])\n",
            "hidden shape after linear: torch.Size([10, 5])\n",
            "hidden shape after relu: torch.Size([10, 5])\n",
            "outputs shape: torch.Size([10, 30])\n",
            "----\n",
            "\n",
            "TOKEN 2\n",
            "input shape: torch.Size([10])\n",
            "embedding shape: torch.Size([10, 5])\n",
            "hidden shape: torch.Size([10, 5])\n",
            "hidden shape after linear: torch.Size([10, 5])\n",
            "hidden shape after relu: torch.Size([10, 5])\n",
            "outputs shape: torch.Size([10, 30])\n",
            "----\n",
            "\n",
            "TOKEN 3\n",
            "input shape: torch.Size([10])\n",
            "embedding shape: torch.Size([10, 5])\n",
            "hidden shape: torch.Size([10, 5])\n",
            "hidden shape after linear: torch.Size([10, 5])\n",
            "hidden shape after relu: torch.Size([10, 5])\n",
            "outputs shape: torch.Size([10, 30])\n",
            "----\n",
            "\n",
            "TOKEN 4\n",
            "input shape: torch.Size([10])\n",
            "embedding shape: torch.Size([10, 5])\n",
            "hidden shape: torch.Size([10, 5])\n",
            "hidden shape after linear: torch.Size([10, 5])\n",
            "hidden shape after relu: torch.Size([10, 5])\n",
            "outputs shape: torch.Size([10, 30])\n",
            "----\n",
            "\n",
            "total number of outputs: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6. Examining the outputs -- predictions of next word"
      ],
      "metadata": {
        "id": "brC-pZO0L4ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `outs` = a list that has one output for each token in the sequence (here, 4 outputs because there are 4 tokens in each sequence)\n",
        "  - There will be the same number of outputs as there are tokens in the sequence length\n",
        "- The output from each token are the predicted y values for the next token\n",
        "  - Shape: [batch size, vocab size]\n",
        "  - One \"row\" for each sample in the batch\n",
        "  - Each \"column\" refers to a token in the vocab"
      ],
      "metadata": {
        "id": "ayy0Ev5j14Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# outs -- list of all outputs (one per token in the sequence length)\n",
        "print(len(outs))\n",
        "\n",
        "# output from one token (token 1)\n",
        "  # 10 \"rows\" for the 10 samples in the batch\n",
        "  # 30 \"columns\" because there's 30 tokens in our vocab\n",
        "print(outs[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoIHFTHn14zK",
        "outputId": "f7d8f24b-fd20-43b7-8882-68865075d9d8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "torch.Size([10, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rearrange the outputs\n",
        "- Stack all of the outputs together into one tensor\n",
        "  - Shape: [batch size, sequence length, vocab size]\n",
        "  - Dimension 0 = each sample in the batch\n",
        "  - Dimension 1 = the location in the sequence (0-sl)\n",
        "  - Dimension 2 = each token in the vocab\n",
        "- Each of the items in dimension 0 (a sample) now corresponds to the predictions for one sample\n",
        "  - Predictions for each location in the sequence\n",
        "  - Predictions for every token in the vocab"
      ],
      "metadata": {
        "id": "Hk7Bfr4y26jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stacked outputs in a single tensor\n",
        "stacked_outs = torch.stack(outs, dim=1)\n",
        "print(stacked_outs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYJVVkFwIbcc",
        "outputId": "ba050226-b90d-40ba-988e-456ac5df27a6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample 1\n",
        "print(stacked_outs[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac4lvHfGMeY3",
        "outputId": "c33d8f28-5d72-467e-fd89-7f1bce9548f0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss: Cross entropy"
      ],
      "metadata": {
        "id": "VNPA8gLeJPgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target values (the true Y values)\n",
        "The true next token in the sequence"
      ],
      "metadata": {
        "id": "C9t0hce8NGr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall x (shape = [batch size, sequence length])\n",
        "print(x.shape)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ1xV4-Z4LsT",
        "outputId": "e3c32957-f3ea-4a19-ff94-a36cde7da983"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  1],\n",
              "        [ 1,  0, 29,  0],\n",
              "        [28, 24,  2,  1],\n",
              "        [28, 22,  4,  1],\n",
              "        [28, 20,  6,  1],\n",
              "        [ 6,  1,  4, 29],\n",
              "        [ 8,  1,  4, 29],\n",
              "        [ 1,  5, 29,  7],\n",
              "        [ 1,  6, 29,  5],\n",
              "        [28,  2,  1,  7]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall y (shape = [batch_size, sequence_length])\n",
        "print(y.shape)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxoV6UN0JQ6Q",
        "outputId": "f0c3472c-03d5-47ac-e16d-9fd8c5876f10"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  1,  3],\n",
              "        [ 0, 29,  0, 28],\n",
              "        [24,  2,  1,  0],\n",
              "        [22,  4,  1,  2],\n",
              "        [20,  6,  1,  3],\n",
              "        [ 1,  4, 29,  2],\n",
              "        [ 1,  4, 29,  9],\n",
              "        [ 5, 29,  7, 28],\n",
              "        [ 6, 29,  5, 28],\n",
              "        [ 2,  1,  7, 29]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten y values to be a vector (so can do cross entropy on them)\n",
        "# shape: [batch size*sequence length]\n",
        "flattened_y = y.view(-1)\n",
        "print(flattened_y.shape)\n",
        "flattened_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EynkpvrGKlFL",
        "outputId": "e0e953e4-6fa1-4915-a936-364f6bf2439a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  1,  3,  0, 29,  0, 28, 24,  2,  1,  0, 22,  4,  1,  2, 20,  6,  1,  3,  1,  4, 29,  2,  1,  4, 29,  9,  5, 29,  7, 28,  6, 29,  5, 28,  2,  1,  7, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicted Y values (outputs of the RNN)\n",
        "- Shape = [batch size, sequence length, vocab size]\n",
        "- dimension 0 = each sample in the batch size\n",
        "- dimension 1 = the location in the sequence (0-sl)\n",
        "- dimension 2 = each token in the vocab"
      ],
      "metadata": {
        "id": "x0ayNko9NJDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall RNN outputs = predicted y values\n",
        "print(stacked_outs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaA6xPNpNKoE",
        "outputId": "f253b352-ded3-4c6d-affc-112da4d871e6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten these values to be a matrix (so can compare in cross entropy function below)\n",
        "# shape: [(batch size*sequence length), vocab size]\n",
        "flattened_outs = stacked_outs.view(-1, len(vocab))\n",
        "flattened_outs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56hf5_vbJZ4t",
        "outputId": "77ec8a85-0cca-4cb6-c412-c4c915467e8f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross entropy loss"
      ],
      "metadata": {
        "id": "WWv66tKVN39R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = F.cross_entropy(flattened_outs, flattened_y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CwMXExXN40f",
        "outputId": "9cd45536-d93f-4da0-9931-85850591eb0a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.5575, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model class for all steps\n",
        "Recall general deep learning steps:\n",
        "- Get data\n",
        "- Initialize params\n",
        "- Run inputs through RNN architecture\n",
        "- Get outputs\n",
        "- Get loss\n",
        "- Optimize with SGD and learning rate\n",
        "- Step parameters\n",
        "- Iterate"
      ],
      "metadata": {
        "id": "yFm84TYTwPXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "FpHhd7NDvLlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make sequence length longer and batch size bigger\n",
        "sl = 16\n",
        "bs = 64\n",
        "\n",
        "# create sequences (same process as above)\n",
        "new_seqs = L(\n",
        "  (tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "  for i in range(0,len(nums)-sl-1,sl)\n",
        ")\n",
        "\n",
        "# train / valid split\n",
        "cut = int(len(new_seqs) * 0.8)\n",
        "\n",
        "# data loaders\n",
        "new_dls = DataLoaders.from_dsets(\n",
        "  group_chunks(new_seqs[:cut], bs), # at each epoch, the model will see a chunk of contiguous text of size sl*m on each line of the batch\n",
        "  group_chunks(new_seqs[cut:], bs),\n",
        "  bs=bs, drop_last=True, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "X0bFmZoLoEu_"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show sample\n",
        "x,y = first(new_dls.train)\n",
        "for i in range(len(x[:3])):\n",
        "  print(\"X:\", x[i])\n",
        "  print(\"Y:\", y[i])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6rupelXvUf-",
        "outputId": "2eb1dd78-e76d-486a-eb23-79527e5c6ea2"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: tensor([0, 1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1])\n",
            "Y: tensor([1, 2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1, 9])\n",
            "\n",
            "X: tensor([ 2, 28, 11,  1,  2, 28, 12,  1,  2, 28, 13,  1,  2, 28, 14,  1])\n",
            "Y: tensor([28, 11,  1,  2, 28, 12,  1,  2, 28, 13,  1,  2, 28, 14,  1,  2])\n",
            "\n",
            "X: tensor([ 6,  1,  3, 28, 25,  7,  1,  3, 28, 25,  8,  1,  3, 28, 25,  9])\n",
            "Y: tensor([ 1,  3, 28, 25,  7,  1,  3, 28, 25,  8,  1,  3, 28, 25,  9,  1])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model class"
      ],
      "metadata": {
        "id": "Wcg7ziu86SyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel4(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden):\n",
        "    \"\"\"\n",
        "    Initialize layers and hidden state\n",
        "    \"\"\"\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.h_h = nn.Linear(n_hidden, n_hidden)\n",
        "    self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "    self.h = 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Get activations (predicted tokens)\n",
        "    \"\"\"\n",
        "    # initialize outputs\n",
        "    outs = []\n",
        "\n",
        "    # each hidden layer = sl words in the x sequences\n",
        "    for i in range(sl):\n",
        "      # get embeddings for the hidden layers\n",
        "      self.h = self.h + self.i_h(x[:,i])\n",
        "\n",
        "      # get activations\n",
        "      self.h = F.relu(self.h_h(self.h))\n",
        "\n",
        "      # get outputs\n",
        "      out = self.h_o(self.h)\n",
        "      outs.append(out)\n",
        "\n",
        "    # detach gradient -- remove its history (for memory size purposes)\n",
        "    self.h = self.h.detach()\n",
        "\n",
        "    # stack outputs\n",
        "    outs = torch.stack(outs, dim=1)\n",
        "    return outs\n",
        "\n",
        "  # function to reset the hidden state\n",
        "  def reset(self):\n",
        "    self.h = 0"
      ],
      "metadata": {
        "id": "YQYT3sqeaSA5"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create loss function that includes a flattening step\n",
        "This is what we did above"
      ],
      "metadata": {
        "id": "jbS-l1Cn6ZvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(preds, targets):\n",
        "  loss = F.cross_entropy(\n",
        "    preds.view(-1, len(vocab)), # reshape preds to have 2 dimensions: (batch size*sequence length) x vocab size\n",
        "    targets.view(-1) # reshape targets to have 1 dimension: (batch size*sequence length)\n",
        "  )\n",
        "  return loss"
      ],
      "metadata": {
        "id": "hX5B1_ziOyr3"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create learner, train, fit"
      ],
      "metadata": {
        "id": "rc7st-1E6my-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 64\n",
        "\n",
        "learn = Learner(\n",
        "  new_dls,\n",
        "  LMModel4(len(vocab), n_hidden),\n",
        "  loss_func=loss_func,\n",
        "  metrics=accuracy,\n",
        "  cbs=ModelResetter\n",
        ")\n",
        "\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "metadata": {
        "id": "WaB2TVkNyeyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "39c38814-e0b9-4f6d-8f8c-6fed09794376"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.227531</td>\n",
              "      <td>3.086109</td>\n",
              "      <td>0.226156</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.307408</td>\n",
              "      <td>1.944043</td>\n",
              "      <td>0.467367</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.749720</td>\n",
              "      <td>1.901680</td>\n",
              "      <td>0.438802</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.488089</td>\n",
              "      <td>1.969942</td>\n",
              "      <td>0.474935</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.307086</td>\n",
              "      <td>2.102713</td>\n",
              "      <td>0.472005</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.200593</td>\n",
              "      <td>2.241389</td>\n",
              "      <td>0.511719</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.091871</td>\n",
              "      <td>2.195455</td>\n",
              "      <td>0.551270</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.001580</td>\n",
              "      <td>2.262634</td>\n",
              "      <td>0.541341</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.912107</td>\n",
              "      <td>2.321158</td>\n",
              "      <td>0.569661</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.858945</td>\n",
              "      <td>2.332030</td>\n",
              "      <td>0.573812</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.808093</td>\n",
              "      <td>2.321511</td>\n",
              "      <td>0.585612</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.759887</td>\n",
              "      <td>2.455033</td>\n",
              "      <td>0.596029</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.730541</td>\n",
              "      <td>2.436553</td>\n",
              "      <td>0.597168</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.710177</td>\n",
              "      <td>2.436204</td>\n",
              "      <td>0.594889</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.697748</td>\n",
              "      <td>2.431266</td>\n",
              "      <td>0.596598</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN 4: Multilayer RNN\n",
        "- Pass the activations from our recurrent neural network into a second recurrent neural network\n",
        "- Note -- this still suffers from the \"vanishing gradients\" aka \"exploding gradients\" problem. We will address this when we use an LSTM below."
      ],
      "metadata": {
        "id": "1al98TYvRfQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "lvtLpS32xbgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dls(sl, bs, nums):\n",
        "  \"\"\"\n",
        "  In the book, they use the same number (64) for the batch size and number of hidden dimensions. I find that confusing.\n",
        "  I'm making this function to easily go back and forth between dimensions we set and what the book says for results comparison.\n",
        "  \"\"\"\n",
        "  # create sequences (same process as above)\n",
        "  new_seqs = L(\n",
        "    (tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "    for i in range(0,len(nums)-sl-1,sl)\n",
        "  )\n",
        "\n",
        "  # train / valid split\n",
        "  cut = int(len(new_seqs) * 0.8)\n",
        "\n",
        "  # data loaders\n",
        "  dls = DataLoaders.from_dsets(\n",
        "    group_chunks(new_seqs[:cut], bs), # at each epoch, the model will see a chunk of contiguous text of size sl*m on each line of the batch\n",
        "    group_chunks(new_seqs[cut:], bs),\n",
        "    bs=bs, drop_last=True, shuffle=False\n",
        "  )\n",
        "  return dls"
      ],
      "metadata": {
        "id": "e0ndh8HF0Y09"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small size (easier to understand what's happening)\n",
        "sl = 4\n",
        "bs = 10\n",
        "\n",
        "dls = create_dls(sl, bs, nums)\n",
        "\n",
        "# first batch of x and y\n",
        "x,y = first(dls.train)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83h_QdMBxiiG",
        "outputId": "bc2293e6-1075-4228-c68f-0c69d26a5a24"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  1],\n",
              "        [ 1,  0, 29,  0],\n",
              "        [28, 24,  2,  1],\n",
              "        [28, 22,  4,  1],\n",
              "        [28, 20,  6,  1],\n",
              "        [ 6,  1,  4, 29],\n",
              "        [ 8,  1,  4, 29],\n",
              "        [ 1,  5, 29,  7],\n",
              "        [ 1,  6, 29,  5],\n",
              "        [28,  2,  1,  7]])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Initialize layers and hidden state"
      ],
      "metadata": {
        "id": "UVryiU5ESWRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding layer\n",
        "- Creates the embeddings for each token of x"
      ],
      "metadata": {
        "id": "NBtTsiuu7z8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sz = len(vocab)\n",
        "n_hidden = 5\n",
        "\n",
        "emb_layer = nn.Embedding(vocab_sz, n_hidden)\n",
        "emb_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C72kB9hFye1a",
        "outputId": "ae75a5f3-dfd5-4c8d-adcc-2cbf5f4e1123"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hidden (RNN) layer\n",
        "- Previously, the hidden layer was linear\n",
        "- This RNN layer creates activations for the next word in the sequence\n",
        "- RNN inputs: [batch size, sequence length, input size]\n",
        "- RNN outputs: [batch size, sequence length, output size]"
      ],
      "metadata": {
        "id": "fce-d0__735F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 2\n",
        "\n",
        "rnn_layer = nn.RNN(\n",
        "  n_hidden, # number of features in the inputs\n",
        "  n_hidden, # number of features in the hidden state -- the outputs of this layer (here, same as inputs)\n",
        "  num_layers = n_layers, # number of recurrent layers\n",
        "  batch_first=True # has to do with the shape of the inputs\n",
        ")\n",
        "\n",
        "print('Structure:\\t',rnn_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8rOHoF1UbAQ",
        "outputId": "fe635048-1dd4-40a2-b2c6-318066f423e6"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure:\t RNN(5, 5, num_layers=2, batch_first=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output (linear) layer\n",
        "- Creates the predictions of the next word (one prediction for each token in the vocab)"
      ],
      "metadata": {
        "id": "MIJuRRLK8gTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "print('Structure:\\t',output_layer)\n",
        "print('Weights shape:\\t',output_layer.weight.shape)\n",
        "print('Bias shape:\\t',output_layer.bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZwxbdfYUa6S",
        "outputId": "9c7e41c4-cb8a-479f-ec24-f954d6e19ee4"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure:\t Linear(in_features=5, out_features=30, bias=True)\n",
            "Weights shape:\t torch.Size([30, 5])\n",
            "Bias shape:\t torch.Size([30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hidden State\n",
        "- Shape = [number of layers, batch size, number of hidden dimensions]\n",
        "- All 0s"
      ],
      "metadata": {
        "id": "hh2peOZ38zr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = torch.zeros(n_layers, bs, n_hidden)\n",
        "print(hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4eLiWOVUa3h",
        "outputId": "5c59c556-75b0-493d-c5e9-a761304793d6"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Get Y Preds using the RNN architecture\n",
        "Note: don't need to loop over layers here -- it does it all at once!"
      ],
      "metadata": {
        "id": "Y5teInn2SYRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input\n",
        "- Shape = [bs, sl]"
      ],
      "metadata": {
        "id": "oPZ6Iov89VhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw input = ALL of x\n",
        "input = x\n",
        "print(input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHmCMY1BYKvt",
        "outputId": "1fdb1c03-e85e-4723-87fc-c814c432bf65"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get embeddings\n",
        "- Note: don't need to add the embeddings to hidden state -- the RNN will do something different!\n",
        "- Shape = [bs, sl, n hidden]"
      ],
      "metadata": {
        "id": "GyTKKsnU9Xdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb = emb_layer(input)\n",
        "print(emb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D-UanuPye3g",
        "outputId": "f641e48a-21f7-4c39-852d-73bd47bebb06"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run RNN\n",
        "- Inputs\n",
        "  - Embeddings of the raw x: [bs, sl, n_hidden]\n",
        "  - Hidden state: [rnn_layers, bs, n_hidden]\n",
        "- Outputs\n",
        "  - Activations: [bs, sl, n_hidden]\n",
        "  - Updated hidden state: [rnn_layers, bs, n_hidden]"
      ],
      "metadata": {
        "id": "cbOYkbgV99M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results, hidden = rnn_layer(emb, hidden)\n",
        "print(\"activations (results) shape:\", results.shape)\n",
        "print(\"updated hidden state shape:\", hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_djLRziXpFw",
        "outputId": "793e2b53-4f70-4a69-918e-9154cf912db1"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations (results) shape: torch.Size([10, 4, 5])\n",
            "updated hidden state shape: torch.Size([2, 10, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Y preds (with the output layer)\n",
        "- Preds shape: [bs, sl, vocab size]\n",
        "- dim 0 = each sample in the batch\n",
        "- dim 1 = the location in the sequence (0-sl)\n",
        "- dim 2 = each token in the vocab"
      ],
      "metadata": {
        "id": "EB_9Ejsa-jfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = output_layer(results)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKcc7aP3Xo2_",
        "outputId": "3ba92589-05a5-4224-99a0-76b0ca23d200"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Train, fine-tune"
      ],
      "metadata": {
        "id": "P4kl9QBg_IHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model architecture as a class"
      ],
      "metadata": {
        "id": "CdCC3YQ4cN7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel5(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "\n",
        "  def forward(self, x):\n",
        "    res,h = self.rnn(self.i_h(x), self.h)\n",
        "    self.h = h.detach()\n",
        "    return self.h_o(res)\n",
        "\n",
        "  def reset(self):\n",
        "    self.h.zero_()"
      ],
      "metadata": {
        "id": "6PTOetrZXo0o"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train!"
      ],
      "metadata": {
        "id": "CTUgPGMEcVwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the book's sl and bs\n",
        "sl = 16\n",
        "bs = 64\n",
        "\n",
        "# dataloaders\n",
        "dls = create_dls(sl, bs, nums)"
      ],
      "metadata": {
        "id": "RdhN_2QE1tbY"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of hidden states and RNN layers\n",
        "n_hidden = 64\n",
        "n_layers = 2\n",
        "\n",
        "# create learner\n",
        "learn = Learner(\n",
        "  dls,\n",
        "  LMModel5(len(vocab), n_hidden, n_layers),\n",
        "  loss_func=CrossEntropyLossFlat(),\n",
        "  metrics=accuracy,\n",
        "  cbs=ModelResetter\n",
        ")\n",
        "\n",
        "# fit\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "metadata": {
        "id": "gf_FeKQSye5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "a5bad9c2-d11d-4248-df98-851382310b2f"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.106834</td>\n",
              "      <td>2.695658</td>\n",
              "      <td>0.446777</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.186411</td>\n",
              "      <td>1.820358</td>\n",
              "      <td>0.471191</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.723488</td>\n",
              "      <td>1.941531</td>\n",
              "      <td>0.335531</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.494312</td>\n",
              "      <td>1.874899</td>\n",
              "      <td>0.452474</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.318809</td>\n",
              "      <td>1.705663</td>\n",
              "      <td>0.506510</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.159192</td>\n",
              "      <td>1.723225</td>\n",
              "      <td>0.539714</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.033712</td>\n",
              "      <td>1.723945</td>\n",
              "      <td>0.545573</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.944427</td>\n",
              "      <td>1.670675</td>\n",
              "      <td>0.554036</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.871110</td>\n",
              "      <td>1.674439</td>\n",
              "      <td>0.564372</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.803593</td>\n",
              "      <td>1.687730</td>\n",
              "      <td>0.568115</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.742367</td>\n",
              "      <td>1.697585</td>\n",
              "      <td>0.569336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.691771</td>\n",
              "      <td>1.697656</td>\n",
              "      <td>0.566976</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.656371</td>\n",
              "      <td>1.705298</td>\n",
              "      <td>0.565430</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.633393</td>\n",
              "      <td>1.710994</td>\n",
              "      <td>0.566081</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.620824</td>\n",
              "      <td>1.706069</td>\n",
              "      <td>0.565837</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Cell (not a whole LSTM model!)\n",
        "**Problem with deep networks: risk of exploding / disappearing activations**\n",
        "  - Deep neural networks = each layer is another matrix multiplication (multiplying numbers and adding them up)\n",
        "  - These deep models can create problems because it requires you to multiply by a matrix many times (get very big or very small results)\n",
        "  - This is a problem because the way computers store numbers (known as \"floating point\") means that they become less and less accurate the further away the numbers get from zero\n",
        "  - This inaccuracy means that often the gradients calculated for updating the weights end up as zero or infinity for deep networks (called \"vanishing / exploding gradients\") --> in SGD, the weights are either not updated at all or jump to infinity. Either way, they won't improve with training\n",
        "- LSTMs (Long Short Term Memory): a type of layer that avoids exploding activations\n",
        "- GRUs (Gated Recurrent Units): another type of layer that avoids exploding activations (not covered here)\n",
        "\n",
        "**LSTMs vs RNNs**\n",
        "- RNNs\n",
        "  - 1 hidden state (the output of the RNN at the previous time step). Goal = predict next token\n",
        "  - It is bad at remembered what happened much earlier in a document\n",
        "- LSTMs\n",
        "  - Have 2 hidden states:\n",
        "  (1) *Hidden state* (confusing name, I know...). Goal = predict next token\n",
        "  (2) *Cell state*. Goal = long and short term memory\n",
        "  - Better at remembering things now!"
      ],
      "metadata": {
        "id": "BXByTk9ydnR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM step by step for single layer\n",
        "*Note: this would never be done in practice, since the outputs of the layers go into other layers (like the multilayered RNN). There would never be a single output like this (this is showing a single loop from one layer).*\n",
        "\n",
        "*But I find that I need to see this to get a sense of what the gates actually do*"
      ],
      "metadata": {
        "id": "2lW_klGSZ6xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make decisions about structure"
      ],
      "metadata": {
        "id": "8WW6hu3KB2Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 10\n",
        "n_hidden = 8\n",
        "n_inputs = 8 # n_inputs is the same as n_hidden for our example (because of the way we defined the embeddings)\n",
        "vocab_sz = len(vocab)\n",
        "\n",
        "# n_layers = 1 # not using rnn layers for this example\n",
        "sl = 4 # not using sequences for this example, but used to create the data below"
      ],
      "metadata": {
        "id": "-hCJ_wCqm26d"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# back to small version of data\n",
        "dls = create_dls(sl, bs, nums)\n",
        "x,y = first(dls.train)\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQzqs2ti64Ep",
        "outputId": "b658c35c-ce7e-4ccb-9587-cd4f4b90690a"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 4]), torch.Size([10, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize hidden and cell states\n",
        "- Hidden state\n",
        "  - Shape: [bs, n hidden]\n",
        "  - Focus: the NEXT token to predict\n",
        "  - Goals: have the right information for the output layer to predict the correct next token; retain memory of everything that happened in the sentence\n",
        "- Cell state\n",
        "  - Shape: [bs, n hidden]\n",
        "  - Focus: keep long short-term memory\n",
        "  - Goals: longer-term state\n",
        "- (note: only one layer for now)"
      ],
      "metadata": {
        "id": "GvpeVyEPcK8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initilize hidden state and cell state to have 0s\n",
        "state = [torch.zeros(bs, n_hidden) for _ in range(2)]\n",
        "hidden, cell = state[0], state[1]\n",
        "\n",
        "print(hidden.shape)\n",
        "print(cell.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA0DFAfCDpT1",
        "outputId": "14548136-ac7b-432e-8560-7a0bfbecec9a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8])\n",
            "torch.Size([10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inputs"
      ],
      "metadata": {
        "id": "ADS-rsJIcH-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw x input -- one column\n",
        "i = 0\n",
        "input = x[:,i]\n",
        "input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS8W4OPGcUm3",
        "outputId": "183c6a4b-e115-42df-8503-dee6806494ed"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10])"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define embedding structure\n",
        "input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
        "input_to_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBV6DXL0aWr0",
        "outputId": "8aa8e937-bf1c-443a-9855-272283f2f0ff"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get embeddings\n",
        "emb = input_to_hidden(input)\n",
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0XCQhi4cW2W",
        "outputId": "2ed4f7fe-1467-4ca0-9d00-042554ba93da"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine embeddings with (initialized or previous) hidden state\n",
        "- Stack hidden state and embeddings in one big tensor\n",
        "- Because they are stacked, the dimension of the embeddings (n_inputs) can be different from the dimension of the hidden state (n_hidden)"
      ],
      "metadata": {
        "id": "a1tBZ06vgbv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_hidden = torch.cat([hidden, emb], dim=1)\n",
        "updated_hidden.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzF5Lc3zZlrr",
        "outputId": "3861aa3a-8bb7-4e96-9193-81511b50f4a7"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: as we'll see below, all the neural nets (gates) are linear layers with (n_inputs + n_hidden) inputs and (n_hidden) outputs.**"
      ],
      "metadata": {
        "id": "PpkRqury7Y4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forget Gate\n",
        "- NN (linear layer, followed by sigmoid)\n",
        "- Inputs -- (updated) hidden layer\n",
        "- Outputs -- scalars between 0 and 1\n",
        "- Decides which information to keep vs. throw away"
      ],
      "metadata": {
        "id": "d4wk2U6xhIq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forget gate\n",
        "forget_gate = nn.Linear(n_inputs + n_hidden, n_hidden) # initialize params\n",
        "lin = forget_gate(updated_hidden) # linear layer\n",
        "forget_gate_outputs = torch.sigmoid(lin) # sigmoid\n",
        "\n",
        "forget_gate_outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xZjE8i6ZlpL",
        "outputId": "63d9c860-0a12-4c5a-ab71-671f5c4dde75"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update Cell State\n",
        "- Multiply the forget gate outputs by the cell state -- discard values closer to 0, keep values closer to 1\n",
        "- Gives the LSTM the ability to forget things about its long-term state (e.g., if coming across an `xxbos` token or a `.` in this example)"
      ],
      "metadata": {
        "id": "czunGKvQRLe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_cell = cell * forget_gate_outputs\n",
        "updated_cell.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrCaC2fhaJ8Q",
        "outputId": "7715f73b-5e42-474c-c2b2-d06e317e380c"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input Gate\n",
        "- NN (linear layer, followed by sigmoid)\n",
        "- *Note: this is the same process as forget gate (but the linear functions have different initialized parameters)*\n",
        "- Inputs -- (updated) hidden layer\n",
        "- Outputs -- scalars between 0 and 1\n",
        "- Decides which elements of the cell state to update (values close to 1) or not (values close to 0)"
      ],
      "metadata": {
        "id": "JxWYCMEAijT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input gate\n",
        "input_gate = nn.Linear(n_inputs + n_hidden, n_hidden) # initialize params\n",
        "lin = input_gate(updated_hidden) # linear layer\n",
        "input_gate_outputs = torch.sigmoid(lin) # sigmoid\n",
        "\n",
        "input_gate_outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ragcfnVqHO7X",
        "outputId": "afde40e3-e55c-4614-dbb7-a3e4ee0c5228"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cell gate\n",
        "- NN (linear layer, followed by tanh). tanh = sigmoid function rescaled to the range -1 to 1\n",
        "- Inputs -- (updated) hidden layer\n",
        "- Outputs -- numbers with range of -1 to +1\n",
        "- If the input gate thinks a value should be updated, the cell gate determines what the updated values actually are"
      ],
      "metadata": {
        "id": "QdcSaxuJRaOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cell_gate = nn.Linear(n_inputs + n_hidden, n_hidden) # initialize params\n",
        "lin = cell_gate(updated_hidden) # linear layer\n",
        "cell_gate_outputs = torch.tanh(lin) # tanh\n",
        "\n",
        "cell_gate_outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b6f6bKhaJ0D",
        "outputId": "c6ea2107-f872-42be-8288-aa6f50bbb52c"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update Cell State\n",
        "- Multiply the input gate outputs and the cell gate outputs\n",
        "- Add these products to the updated cell (that was updated by the forget gate)\n",
        "- Lets the LSTM replace information if need be (e.g., replace a gender pronoun if the forget gate removed it)"
      ],
      "metadata": {
        "id": "R14o_Z8wSP7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "updated_cell2 = updated_cell + input_gate_outputs * cell_gate_outputs\n",
        "print(updated_cell2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7jRV1VUaJxt",
        "outputId": "be198d97-be5c-4c5c-8647-53c052756d7d"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output gate\n",
        "- NN (linear layer, followed by a sigmoid)\n",
        "- Inputs -- (updated) hidden layer\n",
        "- Outputs -- scalars between 0 and 1\n",
        "- Determines which information to use from the cell state to generate the output"
      ],
      "metadata": {
        "id": "fOerVOR5kNCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_gate = nn.Linear(n_inputs + n_hidden, n_hidden) # initialize params\n",
        "lin = output_gate(updated_hidden) # linear layer\n",
        "output_gate_outputs = torch.sigmoid(lin) # sigmoid\n",
        "\n",
        "output_gate_outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_dfb_DeaJpI",
        "outputId": "501ee1ce-c7c4-4d68-8a8c-65c422a01914"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update hidden and cell states\n",
        "Updated hidden state is also the output"
      ],
      "metadata": {
        "id": "-n94g07CTjSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update cell state with tanh -- this is the new cell state!\n",
        "new_cell_state = torch.tanh(updated_cell2)\n",
        "new_cell_state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naPqT_METtNj",
        "outputId": "d2fc3f45-0d0d-4ded-abee-ea400a2de872"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update hidden state by combining updated cell state with results from output gate -- this is the new hidden state!\n",
        "new_hidden_state = output_gate_outputs * new_cell_state\n",
        "new_hidden_state.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTQaqIZhaPdy",
        "outputId": "336069ff-4fd4-40bb-8d8a-f72fcd83ce9d"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keep track of the new hidden and cell states\n",
        "new_state = (new_hidden_state, new_cell_state)"
      ],
      "metadata": {
        "id": "44o9bKnfUExP"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## All together in class"
      ],
      "metadata": {
        "id": "MwY3yjWZkhXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(Module):\n",
        "  def __init__(self, ni, nh):\n",
        "    # the way we define the embeddings right now, the number of inputs (ni)\n",
        "    # is the same as the number of hidden dimensions (nh)\n",
        "    self.forget_gate = nn.Linear(ni + nh, nh)\n",
        "    self.input_gate  = nn.Linear(ni + nh, nh)\n",
        "    self.cell_gate   = nn.Linear(ni + nh, nh)\n",
        "    self.output_gate = nn.Linear(ni + nh, nh)\n",
        "\n",
        "  def forward(self, input, state):\n",
        "    # same steps as above!\n",
        "    h,c = state\n",
        "    h = torch.cat([h, input], dim=1)\n",
        "    forget = torch.sigmoid(self.forget_gate(h))\n",
        "    c = c * forget\n",
        "    inp = torch.sigmoid(self.input_gate(h))\n",
        "    cell = torch.tanh(self.cell_gate(h))\n",
        "    c = c + inp * cell\n",
        "    out = torch.sigmoid(self.output_gate(h))\n",
        "    h = out * torch.tanh(c)\n",
        "    return h, (h,c)"
      ],
      "metadata": {
        "id": "ie0r1z62aPWw"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# more efficient version (uses parallelization)\n",
        "class LSTMCell(Module):\n",
        "  def __init__(self, ni, nh):\n",
        "    self.ih = nn.Linear(ni,4*nh)\n",
        "    self.hh = nn.Linear(nh,4*nh)\n",
        "\n",
        "  def forward(self, input, state):\n",
        "    h,c = state\n",
        "\n",
        "    # one big multiplication for all the gates is better than 4 smaller ones\n",
        "    gates = (self.ih(input) + self.hh(h)).chunk(4, 1) # chunk -- splits our tensor into four pieces\n",
        "    ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
        "    cellgate = gates[3].tanh()\n",
        "\n",
        "    c = (forgetgate*c) + (ingate*cellgate)\n",
        "    h = outgate * c.tanh()\n",
        "\n",
        "    return h, (h,c)"
      ],
      "metadata": {
        "id": "HX2CEzIVaPUb"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full LSTM Model"
      ],
      "metadata": {
        "id": "-8NvZo6klqvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step by Step"
      ],
      "metadata": {
        "id": "17dDjkmzIPUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize params (in layers and state)"
      ],
      "metadata": {
        "id": "XkVttnG8ssU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make decisions\n",
        "bs = 10\n",
        "n_hidden = 8\n",
        "n_inputs = 8\n",
        "vocab_sz = len(vocab)\n",
        "n_layers = 2 # number of rnn layers\n",
        "sl = 4"
      ],
      "metadata": {
        "id": "ziubp32yI44C"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding layer\n",
        "input_to_hidden = nn.Embedding(vocab_sz, n_hidden)\n",
        "input_to_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPMCMMiTISfh",
        "outputId": "c1389945-d3d7-4c39-b48f-929805204cbc"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm layer (instead of the basic rnn layer)\n",
        "rnn = nn.LSTM(\n",
        "  n_hidden, # number of features in the inputs\n",
        "  n_hidden, # number of features in the hidden state (here, same as inputs)\n",
        "  num_layers = n_layers, # number of recurrent layers\n",
        "  batch_first=True # has to do with the shape of the inputs\n",
        ")\n",
        "\n",
        "rnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdCWcgUvJFGS",
        "outputId": "bbafed50-7b40-44d0-ee78-929665d61a52"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(8, 8, num_layers=2, batch_first=True)"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output layer\n",
        "hidden_to_output = nn.Linear(n_hidden, vocab_sz)\n",
        "hidden_to_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlXJfkf6JFFL",
        "outputId": "765e3768-cdb2-4e48-bdd7-ab85b1e43d79"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=8, out_features=30, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initilize hidden state and cell state to have 0s\n",
        "state = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "hidden, cell = state[0], state[1]\n",
        "hidden.shape, cell.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rBeS0fkJFC7",
        "outputId": "7148cbbc-18bd-4318-d54e-ca60eba9a17f"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 10, 8]), torch.Size([2, 10, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward step\n",
        "Note: don't need to add the embeddings to hidden state -- the LSTM will do something different!"
      ],
      "metadata": {
        "id": "oebuKTQaswrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw input = ALL of x\n",
        "input = x\n",
        "input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed7MyPckJE_b",
        "outputId": "562ef0f9-5cad-4350-feed-22730a54ecc0"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get embeddings\n",
        "emb = input_to_hidden(input)\n",
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOj26HynJE7I",
        "outputId": "e7ba7544-c8b7-4f9c-d139-c0afcc72233d"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 4, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM (a type of RNN) -- two inputs:\n",
        "  # input: embeddings, hidden / cell states\n",
        "  # output: activations, hidden / cell states\n",
        "results, state = rnn(emb, state)\n",
        "print(\"activations shape:\", results.shape)\n",
        "print(\"hidden state shape:\", state[0].shape)\n",
        "print(\"cell state shape:\", state[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGgnRhBLKVw6",
        "outputId": "13da53b8-14cc-4fe9-9f7a-22f0b482936d"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activations shape: torch.Size([10, 4, 8])\n",
            "hidden state shape: torch.Size([2, 10, 8])\n",
            "cell state shape: torch.Size([2, 10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get predicted y values\n",
        "  # dimension 0 = each sample in the batch size\n",
        "  # dimension 1 = the location in the sequence (0-sl)\n",
        "  # dimension 2 = each token in the vocab\n",
        "outputs = hidden_to_output(results)\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16zbx5dOKVuT",
        "outputId": "0b74c747-c064-444b-9621-0baa002887c2"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 4, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# detach\n",
        "state = [state_.detach() for state_ in state]\n",
        "\n",
        "# reset state\n",
        "for state_ in state:\n",
        "  state_.zero_()"
      ],
      "metadata": {
        "id": "ZPb1ZbqrtgSZ"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## As a Class"
      ],
      "metadata": {
        "id": "E-nuCcyQL9Lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel6(Module):\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "\n",
        "    # hidden AND cell states\n",
        "    self.state = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "  def forward(self, x):\n",
        "    res,state = self.rnn(self.i_h(x), self.state)\n",
        "    self.state = [h_.detach() for h_ in state]\n",
        "    return self.h_o(res)\n",
        "\n",
        "  def reset(self):\n",
        "    for state in self.state: state.zero_()"
      ],
      "metadata": {
        "id": "sAQlUhhtKVr8"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# back to big version of data\n",
        "sl, bs = 16, 64\n",
        "dls = create_dls(sl, bs, nums)"
      ],
      "metadata": {
        "id": "w-xvA1awBrxX"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "n_hidden = 64\n",
        "n_layers = 2\n",
        "\n",
        "learn = Learner(\n",
        "  dls,\n",
        "  LMModel6(len(vocab), n_hidden, n_layers),\n",
        "  loss_func=CrossEntropyLossFlat(),\n",
        "  metrics=accuracy,\n",
        "  cbs=ModelResetter\n",
        ")\n",
        "\n",
        "learn.fit_one_cycle(15, 1e-2)"
      ],
      "metadata": {
        "id": "NJCPZl45aPNP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "ea0fd5bc-1739-4e44-9109-7a23febb37ea"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.032064</td>\n",
              "      <td>2.761434</td>\n",
              "      <td>0.294840</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.233891</td>\n",
              "      <td>2.206143</td>\n",
              "      <td>0.216471</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.651446</td>\n",
              "      <td>1.998826</td>\n",
              "      <td>0.393717</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.364812</td>\n",
              "      <td>2.169563</td>\n",
              "      <td>0.469727</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.140272</td>\n",
              "      <td>2.139261</td>\n",
              "      <td>0.509521</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.931072</td>\n",
              "      <td>2.333035</td>\n",
              "      <td>0.518311</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.730960</td>\n",
              "      <td>1.916736</td>\n",
              "      <td>0.541585</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.539785</td>\n",
              "      <td>1.410841</td>\n",
              "      <td>0.636719</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.361171</td>\n",
              "      <td>1.231609</td>\n",
              "      <td>0.691569</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.229380</td>\n",
              "      <td>1.136598</td>\n",
              "      <td>0.714925</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.147895</td>\n",
              "      <td>1.080822</td>\n",
              "      <td>0.728841</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.099299</td>\n",
              "      <td>1.094051</td>\n",
              "      <td>0.752848</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.071996</td>\n",
              "      <td>1.107014</td>\n",
              "      <td>0.747884</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.057212</td>\n",
              "      <td>1.099575</td>\n",
              "      <td>0.755046</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.050359</td>\n",
              "      <td>1.092575</td>\n",
              "      <td>0.756185</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularizing an LSTM\n",
        "- RNNs tend to overfit (even using LSTMs and GRU cells)\n",
        "- Can't really use data augmentation because that requires another model (e.g., translate text into another language and then back to the original language)\n",
        "- Other regularization techniques can be used to reduce overfitting: dropout, activation regularization, temporal activation regularization"
      ],
      "metadata": {
        "id": "CwUpY4daMbqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropout\n",
        "- Randomly change some activations to 0 at training time -- makes sure all neurons actively work toward the output\n",
        "- At test time, you use all activations\n",
        "- Helps with overfitting / generalization -- helps the neurons to cooperate better together, and then it makes the activations more noisy (making the model more robust)\n",
        "- Using dropout before passing the output of the LSTM to the final layer will help reduce overfitting\n",
        "- If you apply dropout with probability `p`, you rescale all activations by dividing them by `1-p` --> on average, p will be zeroed, so it leaves `1-p`\n"
      ],
      "metadata": {
        "id": "2jCL94A3rBQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize"
      ],
      "metadata": {
        "id": "gYjRw-Ytwb69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create sample activations\n",
        "act = tensor(range(1,11)).float()\n",
        "act"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFj_HeKZwi28",
        "outputId": "acf9b1e9-751a-4705-8c8a-9954c4cb2317"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose proportion of activations to dropout (p)\n",
        "p = 0.4"
      ],
      "metadata": {
        "id": "3zuCfa-ewgAU"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose if training (use dropout) or evaluation (no dropout)\n",
        "training=True"
      ],
      "metadata": {
        "id": "CYrtJwEXwtJi"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward step"
      ],
      "metadata": {
        "id": "OAWTX0llwoAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a new tensor (called mask) with the same shape of x\n",
        "mask = act.new(*act.shape)\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faVq8J8ZwqSt",
        "outputId": "23a3698b-b1b3-4f85-b830-dae095e3a38b"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.3759e-34, 3.2165e-41, 0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00, 0.0000e+00, 8.0000e+00, 1.1210e-43, 0.0000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly make some values of mask 0s (with probability p) and the rest 1s (with probability 1-p)\n",
        "mask = mask.bernoulli_(1-p)\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsjobnLNxBMG",
        "outputId": "6598dfe9-c34b-43f9-fe6f-b04e2644d5bb"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 1., 1., 0., 1., 0., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply the inputs (act) and mask together -- keeps some of the original values and replaces others with 0s\n",
        "tmp = act*mask\n",
        "tmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssTiodnXxaJt",
        "outputId": "2d33401e-48d4-474a-9a86-4d26fbabc685"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 4., 5., 0., 7., 0., 9., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide the result by (1-p) -- this helps scale the activations (see below)\n",
        "dropout = act*mask.div_(1-p)\n",
        "dropout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRmeQY5wrDyu",
        "outputId": "12aa5177-cf5f-4e0f-e1b5-6f53e1614111"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000,  0.0000,  0.0000,  6.6667,  8.3333,  0.0000, 11.6667,  0.0000, 15.0000,  0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To understand why the scaling is done (dividing the result by (1-p)\n",
        "`w` is the same for original and rescaled activations, but not the (unscaled) activations with dropout"
      ],
      "metadata": {
        "id": "bMJVT2SdyA3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_values(input):\n",
        "  num_nonzeros = torch.count_nonzero(input) # count number of non-zero elements\n",
        "  s = torch.sum(input) # get the sum of all elements\n",
        "  v = s / num_nonzeros # the \"weight\" of each tensor\n",
        "  w = num_nonzeros * v # the total \"weight\" of all of the non-zero tensors\n",
        "\n",
        "  return num_nonzeros, s, v, w"
      ],
      "metadata": {
        "id": "-SOqs_-jy6e_"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original activations\n",
        "num_nonzeros,s,v,w = get_values(act)\n",
        "\n",
        "print(\"number of non-zeros:\", num_nonzeros)\n",
        "print(\"the sum of all elements:\", s)\n",
        "print(\"the weight of each tensor:\", v)\n",
        "print(\"the total weight of all the non-zero tensors:\", w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRDlrGqUyS4J",
        "outputId": "e684d1db-d35b-43fa-e767-7f39c3e5f470"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of non-zeros: tensor(10)\n",
            "the sum of all elements: tensor(55.)\n",
            "the weight of each tensor: tensor(5.5000)\n",
            "the total weight of all the non-zero tensors: tensor(55.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# activations with dropout (but no scaling)\n",
        "num_nonzeros,s,v,w = get_values(mask)\n",
        "\n",
        "print(\"number of non-zeros:\", num_nonzeros)\n",
        "print(\"the sum of all elements:\", s)\n",
        "print(\"the weight of each tensor:\", v)\n",
        "print(\"the total weight of all the non-zero tensors:\", w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnyiqPWoyvn9",
        "outputId": "eb9544c8-469c-4f93-aa36-53267b95d43c"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of non-zeros: tensor(4)\n",
            "the sum of all elements: tensor(6.6667)\n",
            "the weight of each tensor: tensor(1.6667)\n",
            "the total weight of all the non-zero tensors: tensor(6.6667)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the rescaled activations (divide each activation by 1-p)\n",
        "num_nonzeros,s,v,w = get_values(dropout)\n",
        "\n",
        "print(\"number of non-zeros:\", num_nonzeros)\n",
        "print(\"the sum of all elements:\", s)\n",
        "print(\"the weight of each tensor:\", v)\n",
        "print(\"the total weight of all the non-zero tensors:\", w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1ZpuIuSr5zl",
        "outputId": "fca70f6e-51d9-4c04-a5e4-d0dbef0e0cbc"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of non-zeros: tensor(4)\n",
            "the sum of all elements: tensor(41.6667)\n",
            "the weight of each tensor: tensor(10.4167)\n",
            "the total weight of all the non-zero tensors: tensor(41.6667)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As a class"
      ],
      "metadata": {
        "id": "NNSxRtxuGCGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Module):\n",
        "  def __init__(self, p):\n",
        "    \"\"\"\n",
        "    Choose proportion of activations to dropout (p)\n",
        "    \"\"\"\n",
        "    self.p = p\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    At evaluation time, use all neurons\n",
        "    During training, only keep some of the neurons (keep 1-p)\n",
        "    \"\"\"\n",
        "    if not self.training: # training attribute is available in any PyTorch nn.Module\n",
        "      return x\n",
        "\n",
        "    # creates a tensor of random 0s (probability p) and 1s (probability 1-p)\n",
        "    # which is then multiplied with the input\n",
        "    mask = x.new(*x.shape).bernoulli_(1-p)\n",
        "\n",
        "    # divide by 1-p\n",
        "    res = x * mask.div_(1-p)\n",
        "    return res"
      ],
      "metadata": {
        "id": "5O4q05DQMdpS"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Regularization\n",
        "- Tries to make the final activations produced by the LSTM as small as possible -- aka regularizes the final activations\n",
        "- Store the activations, then add the means of the squares of them to the loss (along with a multiplier alpha)\n",
        "- `loss += alpha * activations.pow(2).mean()`\n",
        "- Often applied on the DROPPED-OUT activations (to not penalize the activations we turned into zeros afterward)"
      ],
      "metadata": {
        "id": "Oxz2wv0z0Tjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reminder of sample activations (the y preds)\n",
        "print(act)\n",
        "\n",
        "# make true y values half 0s and half ones\n",
        "targets = tensor([0,0,0,0,0,1,1,1,1,1]).float()\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzJBbueB1FlP",
        "outputId": "dd94cadf-6211-44a8-eb82-37007482c66e"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
            "tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the loss\n",
        "loss = F.cross_entropy(act, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU7KnYJT1rwt",
        "outputId": "a3b3c786-afa6-4ed2-8b85-26678e10c169"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(12.2931)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add a penalty to the loss\n",
        "alpha = 2 # hyperparameter we can set (a multiplier)"
      ],
      "metadata": {
        "id": "wtiLGNNd2TXk"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# square each of the values in the activations tensor -- get the mean of these values\n",
        "tmp_act = act.pow(2)\n",
        "tmp_act_mean = tmp_act.mean()\n",
        "\n",
        "print(tmp_act)\n",
        "print(tmp_act_mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdWW3DOU2UuB",
        "outputId": "d779a594-1aa4-4b19-b747-fa6001094ec9"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  1.,   4.,   9.,  16.,  25.,  36.,  49.,  64.,  81., 100.])\n",
            "tensor(38.5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply this value (the mean of the squared activations) by the alpha parameter\n",
        "penalty = alpha * tmp_act_mean\n",
        "print(penalty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH7tXOZj20Vs",
        "outputId": "7743c5b7-1af3-4a3d-cb5b-1773652cdf3b"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(77.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the loss with the penalty\n",
        "updated_loss = loss + penalty\n",
        "print(updated_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvmOcCqu0jlS",
        "outputId": "e7e69ef7-4b73-4bae-cbda-16b02e68e033"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(89.2932)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporal Activation Regularization\n",
        "- Linked to the fact that we are predicting tokens in a sequence -- the outputs of our LSTMs should somewhat make sense when we read them in order\n",
        "- Adds a penalty to the loss to make the difference between two consecutive activations as small as possible\n",
        "- `loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()`\n",
        "- Applied on the NON-DROPPED-OUT activations (because those zeros create big differences between two consecutive time steps)"
      ],
      "metadata": {
        "id": "kWOPOVdq2de-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall original y\n",
        "print(y.shape) # bs x sl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZEEUszT3gaw",
        "outputId": "97b6744a-b00b-4f14-997b-795196d0be5a"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall activations (from the lstm)\n",
        "activ, state = rnn(emb, state)\n",
        "print(activ.shape) # bs x sl x n_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYRAuw7p20Fz",
        "outputId": "e9ad558f-ce53-4a96-e30e-70233179c061"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall: each x consists of a sequence of 4 tokens\n",
        "# note: the dimension in the middle (of the activations) has to do with the location in the sequence\n",
        "# for example, this gets all of the activations that correspond to the FIRST word in every x sequence\n",
        "first_token = activ[:,0,:]\n",
        "print(first_token.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZd7rC2p3sr_",
        "outputId": "ddbd607e-fa33-4b76-aacf-aebf51750a6d"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this gets all of the activations that correspond to the SECOND word in every x sequence\n",
        "second_token = activ[:,1,:]\n",
        "print(second_token.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFbMMXjZ3ewo",
        "outputId": "cb31650d-0823-4d04-b294-5e44171788d2"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the activations for the second, third, AND fourth tokens (in each x)\n",
        "tokens_234 = activ[:,1:,:]\n",
        "print(tokens_234.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3j174J73y_m",
        "outputId": "95904326-6a64-47f2-ff44-2841780bf635"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the activations for the first, second, AND third tokens (in each x)\n",
        "tokens_123 = activ[:,:-1,:]\n",
        "print(tokens_123.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjhYnuOF5wEs",
        "outputId": "370ef9b1-f814-4c35-92e2-1e1cab802e20"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the loss\n",
        "# recall outputs have shape bs x sl x vocab size\n",
        "loss = F.cross_entropy(\n",
        "  outputs.view(-1, len(vocab)), # reshape preds to have 2 dimensions: (batch size*sequence length) x vocab size\n",
        "  y.view(-1) # reshape targets to have 1 dimension: (batch size*sequence length)\n",
        ")\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N8ylB6u79-1",
        "outputId": "bb12f64d-79ef-4490-b32e-7de291a9f4ae"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.4257, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the penalty to the loss will make the difference between two consecutive activations as small as possible\n",
        "# add a penalty to the loss\n",
        "beta = 1 # hyperparameter we can set (a multiplier)"
      ],
      "metadata": {
        "id": "DqaQMSW136I1"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporal activation regularization wants to minimize the DIFFERENCE between two tokens\n",
        "diff = (tokens_234 - tokens_123)\n",
        "print(diff.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X98WCm5X37X_",
        "outputId": "fb211467-9df1-4770-c735-917221e45e88"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# square each of the values in the activations tensor -- get the mean of these values\n",
        "tmp_act = diff.pow(2).mean()\n",
        "print(tmp_act)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmJIMa4b39M6",
        "outputId": "757902f3-8962-45d0-c9a7-e016aebf035b"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0002, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# multiply this value (the mean of the squared activations) by the alpha parameter\n",
        "penalty = beta * tmp_act\n",
        "print(penalty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yStTz8o_4FYV",
        "outputId": "232a6830-1650-404f-8a8c-e34dad58a93d"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0002, grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the loss with the penalty\n",
        "updated_loss = loss + penalty\n",
        "print(updated_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBpdjcWVwb-T",
        "outputId": "715ef0f5-a69e-4e96-a9ce-f503b0ec68c7"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.4259, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight-Tied Regularized LSTM\n",
        "- In a language model, the input embeddings represent a mapping from English words to activations, and the output hidden layer represents a mapping from activations to English words\n",
        "- We might expect, intuitively, that these mappings could be the same\n",
        "- We can represent this in PyTorch by assigning the same weight matrix to each of these layers: `self.h_o.weight = self.i_h.weight`\n",
        "- Weight tying: make the parameter matrices for the embeddings (i_h) the same as the preds for tokens (h_o)"
      ],
      "metadata": {
        "id": "ORnIPfxX9dBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LMModel7(Module):\n",
        "  \"\"\"\n",
        "  Includes dropout and weight tying\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
        "    self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "    self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "    self.drop = nn.Dropout(p) # proportion of activations to drop out (prevent overfitting)\n",
        "    self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "    self.h_o.weight = self.i_h.weight # weight typing\n",
        "    self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "\n",
        "  def forward(self, x):\n",
        "    raw,h = self.rnn(self.i_h(x), self.h)\n",
        "    out = self.drop(raw)\n",
        "    self.h = [h_.detach() for h_ in h]\n",
        "    return self.h_o(out),raw,out\n",
        "\n",
        "  def reset(self):\n",
        "    for h in self.h: h.zero_()"
      ],
      "metadata": {
        "id": "QPdN6KPVwb53"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNRegularizer applies activation regularization (AR) and temporal activation regularization (TAR)\n",
        "  # alpha = for AR\n",
        "  # beta = for TAR\n",
        "batch_size = 64\n",
        "n_rnn_layers = 2\n",
        "dropout_prop = 0.4\n",
        "\n",
        "learn = Learner(\n",
        "  dls,\n",
        "  LMModel7(len(vocab), batch_size, n_rnn_layers, dropout_prop),\n",
        "  loss_func=CrossEntropyLossFlat(),\n",
        "  metrics=accuracy,\n",
        "  cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)]\n",
        ")"
      ],
      "metadata": {
        "id": "qfcOF5XRwb3R"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this learner does the same thing as the learner above\n",
        "# but as a text learner (does the two callbacks automatically)\n",
        "learn = TextLearner(\n",
        "  dls,\n",
        "  LMModel7(len(vocab), batch_size, n_rnn_layers, dropout_prop),\n",
        "  loss_func=CrossEntropyLossFlat(),\n",
        "  metrics=accuracy\n",
        ")"
      ],
      "metadata": {
        "id": "Q8penU2Wwbwn"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add even more regularization when training (weight decay)\n",
        "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
      ],
      "metadata": {
        "id": "vKgwgnBp--I3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "444f72b5-412b-4d6a-cd90-70519dde0ab1"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.639513</td>\n",
              "      <td>2.030643</td>\n",
              "      <td>0.456380</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.584239</td>\n",
              "      <td>1.332765</td>\n",
              "      <td>0.689697</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.842080</td>\n",
              "      <td>0.922213</td>\n",
              "      <td>0.791911</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.421628</td>\n",
              "      <td>0.691511</td>\n",
              "      <td>0.815674</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.220742</td>\n",
              "      <td>0.668626</td>\n",
              "      <td>0.837891</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.124636</td>\n",
              "      <td>0.428851</td>\n",
              "      <td>0.873210</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.075268</td>\n",
              "      <td>0.375319</td>\n",
              "      <td>0.878418</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.050386</td>\n",
              "      <td>0.402634</td>\n",
              "      <td>0.883301</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.036881</td>\n",
              "      <td>0.412301</td>\n",
              "      <td>0.885010</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.028361</td>\n",
              "      <td>0.371559</td>\n",
              "      <td>0.896566</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.023403</td>\n",
              "      <td>0.359768</td>\n",
              "      <td>0.898031</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.019285</td>\n",
              "      <td>0.362903</td>\n",
              "      <td>0.896647</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.016743</td>\n",
              "      <td>0.375941</td>\n",
              "      <td>0.896159</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.014381</td>\n",
              "      <td>0.405771</td>\n",
              "      <td>0.889811</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.013450</td>\n",
              "      <td>0.395382</td>\n",
              "      <td>0.892334</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AWD-LSTM architecture for text classification (from *08_nlp_basics.ipynb*)\n",
        "- Includes more dropout:\n",
        "  - Embedding dropout (inside the embedding layer, drops some random lines of embeddings)\n",
        "  - Input dropout (applied after the embedding layer)\n",
        "  - Weight dropout (applied to the weights of the LSTM at each training step)\n",
        "  - Hidden dropout (applied to the hidden state between two layers)\n",
        "- Since fine-tuning those five dropout values (including the dropout before the output layer) is complicated, fastai has determined good defaults and allow the magnitude of dropout to be tuned overall with the drop_mult parameter we saw in that chapter (which is multiplied by each dropout)."
      ],
      "metadata": {
        "id": "tf30eCK943jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers architecture\n",
        "- Also works works well for \"sequence-to-sequence\" problems (where the dependent variable is itself a variable-length sequence, such as language translation)"
      ],
      "metadata": {
        "id": "9lNoI4915DSR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCgq30mDJR3u"
      },
      "execution_count": 186,
      "outputs": []
    }
  ]
}